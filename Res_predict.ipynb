{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc655f0d-3498-4848-a5d8-34d6847e9fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import stats\n",
    "import time\n",
    "import warnings\n",
    "import os\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class RandomForestWithUncertainty:\n",
    "    \"\"\"\n",
    "    Random Forest regressor that provides uncertainty estimates\n",
    "    for resistivity prediction in hydrologic surveys\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, \n",
    "                 min_samples_leaf=1, random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.random_state = random_state\n",
    "        self.rf = None\n",
    "        self.is_fitted = False\n",
    "        self.feature_scaler = None\n",
    "        self.output_scaler = None\n",
    "        self.log_offset = 0\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Random Forest model with scalers\n",
    "        \"\"\"\n",
    "        self.rf = RandomForestRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            random_state=self.random_state,\n",
    "            oob_score=True,  # Enable out-of-bag scoring\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        self.rf.fit(X, y)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        # Store training data for nearest neighbor uncertainty\n",
    "        self.X_train = X.copy()\n",
    "        self.y_train = y.copy()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_with_uncertainty(self, X, method='combined', confidence_level=0.95):\n",
    "        \"\"\"\n",
    "        Predict resistivity with uncertainty estimates\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Input features (normalized)\n",
    "        method : str, default='combined'\n",
    "            Method for uncertainty estimation:\n",
    "            - 'ensemble_variance': Use variance across trees\n",
    "            - 'nearest_neighbor': Use local prediction variance\n",
    "            - 'combined': Weighted combination of both\n",
    "        confidence_level : float, default=0.95\n",
    "            Confidence level for prediction intervals\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : array, shape (n_samples,)\n",
    "            Mean resistivity predictions\n",
    "        uncertainties : array, shape (n_samples,)\n",
    "            Uncertainty estimates (standard deviation)\n",
    "        prediction_intervals : array, shape (n_samples, 2)\n",
    "            Lower and upper bounds of prediction intervals\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        \n",
    "        if method == 'ensemble_variance':\n",
    "            return self._ensemble_variance_uncertainty(X, confidence_level)\n",
    "        elif method == 'nearest_neighbor':\n",
    "            return self._nearest_neighbor_uncertainty(X, confidence_level)\n",
    "        elif method == 'combined':\n",
    "            return self._combined_uncertainty(X, confidence_level)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown uncertainty method: {method}\")\n",
    "    \n",
    "    def _ensemble_variance_uncertainty(self, X, confidence_level):\n",
    "        \"\"\"\n",
    "        Calculate uncertainty from ensemble variance\n",
    "        \"\"\"\n",
    "        # Get predictions from all trees\n",
    "        tree_predictions = np.array([\n",
    "            tree.predict(X) for tree in self.rf.estimators_\n",
    "        ])\n",
    "        \n",
    "        # Calculate statistics\n",
    "        predictions = np.mean(tree_predictions, axis=0)\n",
    "        uncertainties = np.std(tree_predictions, axis=0)\n",
    "        \n",
    "        # Calculate prediction intervals\n",
    "        z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "        margin = z_score * uncertainties\n",
    "        \n",
    "        prediction_intervals = np.column_stack([\n",
    "            predictions - margin,\n",
    "            predictions + margin\n",
    "        ])\n",
    "        \n",
    "        return predictions, uncertainties, prediction_intervals\n",
    "    \n",
    "    def _nearest_neighbor_uncertainty(self, X, confidence_level, k=10):\n",
    "        \"\"\"\n",
    "        Calculate uncertainty based on local neighborhood variance\n",
    "        \"\"\"\n",
    "        # Fit nearest neighbors on training data\n",
    "        nn = NearestNeighbors(n_neighbors=min(k, len(self.X_train)))\n",
    "        nn.fit(self.X_train)\n",
    "        \n",
    "        # Find nearest neighbors for each prediction point\n",
    "        distances, indices = nn.kneighbors(X)\n",
    "        \n",
    "        # Get base predictions\n",
    "        predictions = self.rf.predict(X)\n",
    "        uncertainties = np.zeros(len(X))\n",
    "        \n",
    "        # Calculate local uncertainty for each point\n",
    "        for i in range(len(X)):\n",
    "            # Get predictions for nearest neighbors\n",
    "            neighbor_indices = indices[i]\n",
    "            if hasattr(self.y_train, 'iloc'):\n",
    "                neighbor_targets = self.y_train.iloc[neighbor_indices]\n",
    "            else:\n",
    "                neighbor_targets = self.y_train[neighbor_indices]\n",
    "            \n",
    "            # Use variance of neighbors + distance penalty as uncertainty estimate\n",
    "            local_variance = np.var(neighbor_targets)\n",
    "            distance_penalty = np.mean(distances[i]) * 0.1  # Scale distance penalty\n",
    "            uncertainties[i] = np.sqrt(local_variance + distance_penalty)\n",
    "        \n",
    "        # Calculate prediction intervals\n",
    "        z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "        margin = z_score * uncertainties\n",
    "        \n",
    "        prediction_intervals = np.column_stack([\n",
    "            predictions - margin,\n",
    "            predictions + margin\n",
    "        ])\n",
    "        \n",
    "        return predictions, uncertainties, prediction_intervals\n",
    "    \n",
    "    def _combined_uncertainty(self, X, confidence_level):\n",
    "        \"\"\"\n",
    "        Combine ensemble variance and nearest neighbor uncertainty\n",
    "        \"\"\"\n",
    "        # Get both uncertainty estimates\n",
    "        pred1, unc1, _ = self._ensemble_variance_uncertainty(X, confidence_level)\n",
    "        pred2, unc2, _ = self._nearest_neighbor_uncertainty(X, confidence_level)\n",
    "        \n",
    "        # Use ensemble prediction (more stable)\n",
    "        predictions = pred1\n",
    "        \n",
    "        # Combine uncertainties (weighted average)\n",
    "        uncertainties = np.sqrt(0.7 * unc1**2 + 0.3 * unc2**2)\n",
    "        \n",
    "        # Calculate prediction intervals\n",
    "        z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "        margin = z_score * uncertainties\n",
    "        \n",
    "        prediction_intervals = np.column_stack([\n",
    "            predictions - margin,\n",
    "            predictions + margin\n",
    "        ])\n",
    "        \n",
    "        return predictions, uncertainties, prediction_intervals\n",
    "    \n",
    "    def identify_high_uncertainty_locations(self, X, uncertainty_threshold_percentile=80):\n",
    "        \"\"\"\n",
    "        Identify locations with high uncertainty for targeted sampling\n",
    "        \"\"\"\n",
    "        predictions, uncertainties, _ = self.predict_with_uncertainty(X, method='combined')\n",
    "        \n",
    "        # Define high uncertainty threshold\n",
    "        threshold = np.percentile(uncertainties, uncertainty_threshold_percentile)\n",
    "        high_uncertainty_mask = uncertainties >= threshold\n",
    "        \n",
    "        # Rank locations by uncertainty\n",
    "        uncertainty_ranking = np.argsort(uncertainties)[::-1]  # Highest first\n",
    "        \n",
    "        return high_uncertainty_mask, uncertainty_ranking, uncertainties\n",
    "\n",
    "# Load the terrain attributes CSV file\n",
    "FILE_PATH = \"./terrain_attributes_clean.csv\"\n",
    "print(f\"Loading terrain data from {FILE_PATH}...\")\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# Explore the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# Identify depth layers (columns starting with 'layer_')\n",
    "depth_layers = [col for col in df.columns if col.startswith('layer_')]\n",
    "print(f\"\\nFound {len(depth_layers)} depth layers: {depth_layers}\")\n",
    "\n",
    "# Feature columns (terrain attributes)\n",
    "feature_columns = ['elevation', 'slope', 'aspect', 'plan_curvature']\n",
    "print(f\"\\nFeature columns: {feature_columns}\")\n",
    "\n",
    "# Function to safely apply log10 transformation to resistivity data\n",
    "def safe_log10_transform(data):\n",
    "    \"\"\"Safely apply log10 transformation to data, handling zeros and negative values.\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    if isinstance(data, pd.Series):\n",
    "        data_copy = data.copy()\n",
    "    else:\n",
    "        data_copy = np.copy(data)\n",
    "    \n",
    "    # Check for non-positive values\n",
    "    min_val = np.nanmin(data_copy)\n",
    "    offset = 0\n",
    "    \n",
    "    # If we have zero or negative values, add an offset\n",
    "    if min_val <= 0:\n",
    "        offset = abs(min_val) + 1e-10  # Add a small epsilon\n",
    "        data_copy = data_copy + offset\n",
    "        print(f\"Added offset of {offset} before log transform (min value was {min_val})\")\n",
    "    \n",
    "    # Apply log10 transform and handle infinities in one step\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        log_data = np.log10(data_copy)\n",
    "        # Replace infinities with NaN\n",
    "        if isinstance(log_data, np.ndarray):\n",
    "            log_data[np.isinf(log_data)] = np.nan\n",
    "        elif isinstance(log_data, pd.Series):\n",
    "            log_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    return log_data, offset\n",
    "\n",
    "# Function to inverse the log10 transformation\n",
    "def inverse_log10_transform(log_data, offset):\n",
    "    \"\"\"Inverse the log10 transformation, removing any offset added.\"\"\"\n",
    "    # Handle infinities\n",
    "    if isinstance(log_data, np.ndarray):\n",
    "        clean_log_data = log_data.copy()\n",
    "        clean_log_data[np.isinf(clean_log_data)] = np.nan\n",
    "    elif isinstance(log_data, pd.Series):\n",
    "        clean_log_data = log_data.replace([np.inf, -np.inf], np.nan)\n",
    "    else:\n",
    "        clean_log_data = log_data\n",
    "        \n",
    "    # Apply inverse transform\n",
    "    return 10**clean_log_data - offset\n",
    "\n",
    "# Function to convert uncertainty to multiple representations (ALL IN ORIGINAL SCALE)\n",
    "def convert_uncertainty_to_original_scale(log_predictions, log_uncertainties, log_offset, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Convert log-space uncertainty to original scale uncertainty measures.\n",
    "    ALL outputs are in original resistivity units (Ω·m) or dimensionless.\n",
    "    \"\"\"\n",
    "    # Z-score for confidence level\n",
    "    z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "    \n",
    "    # Original space predictions\n",
    "    original_predictions = inverse_log10_transform(log_predictions, log_offset)\n",
    "    \n",
    "    # Confidence intervals in log space\n",
    "    log_lower = log_predictions - z_score * log_uncertainties\n",
    "    log_upper = log_predictions + z_score * log_uncertainties\n",
    "    \n",
    "    # Convert to original space\n",
    "    original_lower = inverse_log10_transform(log_lower, log_offset)\n",
    "    original_upper = inverse_log10_transform(log_upper, log_offset)\n",
    "    \n",
    "    # Uncertainty measures in ORIGINAL SCALE\n",
    "    uncertainty_factors = 10**log_uncertainties  # Multiplicative factor (dimensionless)\n",
    "    coefficient_of_variation = (uncertainty_factors - 1) * 100  # Percentage\n",
    "    \n",
    "    # Standard deviation in original space (for log-normal distribution)\n",
    "    # For log-normal: std = mean * sqrt(exp(σ²) - 1) where σ is log-space std\n",
    "    original_std = original_predictions * np.sqrt(np.exp((log_uncertainties * np.log(10))**2) - 1)\n",
    "    \n",
    "    # Alternative: Linear approximation of std in original space\n",
    "    # This is simpler and often more interpretable\n",
    "    linear_std_approx = original_predictions * log_uncertainties * np.log(10)\n",
    "    \n",
    "    # Uncertainty range (half-width of confidence interval)\n",
    "    uncertainty_range = (original_upper - original_lower) / 2\n",
    "    \n",
    "    return {\n",
    "        'uncertainty_std_ohm_m': original_std,  # Standard deviation in Ω·m (exact for log-normal)\n",
    "        'uncertainty_linear_ohm_m': linear_std_approx,  # Linear approximation in Ω·m (simpler)\n",
    "        'uncertainty_range_ohm_m': uncertainty_range,  # Half-width of 95% CI in Ω·m\n",
    "        'uncertainty_factor': uncertainty_factors,  # Multiplicative factor (dimensionless)\n",
    "        'cv_percent': coefficient_of_variation,  # Coefficient of variation (%)\n",
    "        'ci_lower_ohm_m': original_lower,  # Lower confidence interval (Ω·m)\n",
    "        'ci_upper_ohm_m': original_upper,  # Upper confidence interval (Ω·m)\n",
    "        'log_uncertainty_reference': log_uncertainties  # Keep for reference only\n",
    "    }\n",
    "\n",
    "# Function to visualize feature importance for a layer\n",
    "def plot_feature_importance(model, feature_columns, layer_name):\n",
    "    \"\"\"Create a feature importance plot for the trained model.\"\"\"\n",
    "    # Get feature importances\n",
    "    importances = model.rf.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Create a figure for the feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f'Feature Importances for {layer_name}')\n",
    "    plt.bar(range(len(feature_columns)), importances[indices], align='center')\n",
    "    plt.xticks(range(len(feature_columns)), [feature_columns[i] for i in indices], rotation=45)\n",
    "    plt.ylabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f\"{layer_name}_feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved feature importance plot to {layer_name}_feature_importance.png\")\n",
    "\n",
    "# Function to visualize spatial distribution of resistivity with uncertainty\n",
    "def plot_spatial_distribution_with_uncertainty(df, layer_name):\n",
    "    \"\"\"Create comprehensive spatial distribution plots for predictions and uncertainty (ALL IN ORIGINAL SCALE).\"\"\"\n",
    "    uncertainty_cols = {\n",
    "        'std': f\"{layer_name}_uncertainty_std\",           # Ω·m\n",
    "        'linear': f\"{layer_name}_uncertainty_linear\",     # Ω·m (linear approximation)\n",
    "        'cv': f\"{layer_name}_cv_percent\",                 # %\n",
    "        'range': f\"{layer_name}_uncertainty_range\"        # Ω·m (half-width of CI)\n",
    "    }\n",
    "    \n",
    "    # Skip if the layer has too many NaN values\n",
    "    if df[layer_name].isna().sum() > 0.9 * len(df):\n",
    "        print(f\"Too many NaN values to create spatial distribution plot for {layer_name}\")\n",
    "        return\n",
    "    \n",
    "    # Make a copy to avoid modifying original data\n",
    "    plot_df = df.copy()\n",
    "    \n",
    "    # Replace any inf values with NaN\n",
    "    cols_to_clean = [layer_name] + [col for col in uncertainty_cols.values() if col in plot_df.columns]\n",
    "    for col in cols_to_clean:\n",
    "        if col in plot_df.columns:\n",
    "            if isinstance(plot_df[col], pd.Series):\n",
    "                plot_df[col] = plot_df[col].replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Create comprehensive subplot\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot 1: Resistivity predictions (Ω·m)\n",
    "    valid_mask1 = ~plot_df[layer_name].isna()\n",
    "    if valid_mask1.sum() > 0:\n",
    "        sc1 = axes[0].scatter(plot_df.loc[valid_mask1, 'x'], \n",
    "                             plot_df.loc[valid_mask1, 'y'], \n",
    "                             c=plot_df.loc[valid_mask1, layer_name], \n",
    "                             cmap='viridis', alpha=0.7, s=20)\n",
    "        plt.colorbar(sc1, ax=axes[0], label='Resistivity (Ω·m)')\n",
    "        axes[0].set_xlabel('X Coordinate')\n",
    "        axes[0].set_ylabel('Y Coordinate')\n",
    "        axes[0].set_title(f'Resistivity Predictions - {layer_name}')\n",
    "    \n",
    "    # Plot 2: Uncertainty in Ω·m (use std or linear approximation)\n",
    "    uncertainty_col_to_plot = None\n",
    "    uncertainty_label = \"\"\n",
    "    \n",
    "    if uncertainty_cols['std'] in plot_df.columns:\n",
    "        uncertainty_col_to_plot = uncertainty_cols['std']\n",
    "        uncertainty_label = 'Uncertainty Std Dev (Ω·m)'\n",
    "    elif uncertainty_cols['linear'] in plot_df.columns:\n",
    "        uncertainty_col_to_plot = uncertainty_cols['linear']\n",
    "        uncertainty_label = 'Uncertainty Linear Approx (Ω·m)'\n",
    "    elif uncertainty_cols['range'] in plot_df.columns:\n",
    "        uncertainty_col_to_plot = uncertainty_cols['range']\n",
    "        uncertainty_label = 'Uncertainty Range (Ω·m)'\n",
    "    \n",
    "    if uncertainty_col_to_plot:\n",
    "        valid_mask2 = ~plot_df[uncertainty_col_to_plot].isna()\n",
    "        if valid_mask2.sum() > 0:\n",
    "            sc2 = axes[1].scatter(plot_df.loc[valid_mask2, 'x'], \n",
    "                                 plot_df.loc[valid_mask2, 'y'], \n",
    "                                 c=plot_df.loc[valid_mask2, uncertainty_col_to_plot], \n",
    "                                 cmap='Reds', alpha=0.7, s=20)\n",
    "            plt.colorbar(sc2, ax=axes[1], label=uncertainty_label)\n",
    "            axes[1].set_xlabel('X Coordinate')\n",
    "            axes[1].set_ylabel('Y Coordinate')\n",
    "            axes[1].set_title(f'Prediction Uncertainty (Original Scale) - {layer_name}')\n",
    "    \n",
    "    # Plot 3: Coefficient of variation (%)\n",
    "    if uncertainty_cols['cv'] in plot_df.columns:\n",
    "        valid_mask3 = ~plot_df[uncertainty_cols['cv']].isna()\n",
    "        if valid_mask3.sum() > 0:\n",
    "            sc3 = axes[2].scatter(plot_df.loc[valid_mask3, 'x'], \n",
    "                                 plot_df.loc[valid_mask3, 'y'], \n",
    "                                 c=plot_df.loc[valid_mask3, uncertainty_cols['cv']], \n",
    "                                 cmap='Oranges', alpha=0.7, s=20)\n",
    "            plt.colorbar(sc3, ax=axes[2], label='Coefficient of Variation (%)')\n",
    "            axes[2].set_xlabel('X Coordinate')\n",
    "            axes[2].set_ylabel('Y Coordinate')\n",
    "            axes[2].set_title(f'Relative Uncertainty (CV%) - {layer_name}')\n",
    "    \n",
    "    # Plot 4: High uncertainty locations for ModEx (use CV% for prioritization)\n",
    "    if uncertainty_cols['cv'] in plot_df.columns:\n",
    "        valid_mask4 = ~plot_df[uncertainty_cols['cv']].isna()\n",
    "        if valid_mask4.sum() > 0:\n",
    "            # Base scatter plot\n",
    "            sc4 = axes[3].scatter(plot_df.loc[valid_mask4, 'x'], \n",
    "                                 plot_df.loc[valid_mask4, 'y'], \n",
    "                                 c=plot_df.loc[valid_mask4, uncertainty_cols['cv']], \n",
    "                                 cmap='Reds', alpha=0.5, s=15)\n",
    "            \n",
    "            # Identify high uncertainty areas for ModEx\n",
    "            threshold_80 = plot_df[uncertainty_cols['cv']].quantile(0.8)\n",
    "            threshold_90 = plot_df[uncertainty_cols['cv']].quantile(0.9)\n",
    "            \n",
    "            high_unc_80_mask = plot_df[uncertainty_cols['cv']] >= threshold_80\n",
    "            high_unc_90_mask = plot_df[uncertainty_cols['cv']] >= threshold_90\n",
    "            \n",
    "            if high_unc_80_mask.sum() > 0:\n",
    "                axes[3].scatter(plot_df.loc[high_unc_80_mask, 'x'], \n",
    "                               plot_df.loc[high_unc_80_mask, 'y'], \n",
    "                               c='orange', marker='s', s=60, alpha=0.8, \n",
    "                               label=f'Top 20% Uncertainty (n={high_unc_80_mask.sum()})')\n",
    "            \n",
    "            if high_unc_90_mask.sum() > 0:\n",
    "                axes[3].scatter(plot_df.loc[high_unc_90_mask, 'x'], \n",
    "                               plot_df.loc[high_unc_90_mask, 'y'], \n",
    "                               c='red', marker='X', s=100, alpha=0.9,\n",
    "                               label=f'Top 10% Uncertainty (n={high_unc_90_mask.sum()})')\n",
    "            \n",
    "            plt.colorbar(sc4, ax=axes[3], label='Coefficient of Variation (%)')\n",
    "            axes[3].set_xlabel('X Coordinate')\n",
    "            axes[3].set_ylabel('Y Coordinate')\n",
    "            axes[3].set_title(f'ModEx Survey Targets - {layer_name}')\n",
    "            axes[3].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f\"{layer_name}_comprehensive_spatial_analysis.png\", \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved comprehensive spatial analysis to {layer_name}_comprehensive_spatial_analysis.png\")\n",
    "\n",
    "# Function to save optimal survey locations with comprehensive uncertainty info\n",
    "def save_optimal_survey_locations_comprehensive(df, layer_name, n_locations=20):\n",
    "    \"\"\"Identify and save optimal locations for future surveys with all uncertainty measures.\"\"\"\n",
    "    uncertainty_cols = {\n",
    "        'log': f\"{layer_name}_uncertainty_log\",\n",
    "        'cv': f\"{layer_name}_cv_percent\",\n",
    "        'factor': f\"{layer_name}_uncertainty_factor\",\n",
    "        'ci_lower': f\"{layer_name}_ci_lower\",\n",
    "        'ci_upper': f\"{layer_name}_ci_upper\"\n",
    "    }\n",
    "    \n",
    "    if uncertainty_cols['log'] not in df.columns:\n",
    "        print(f\"No uncertainty data available for {layer_name}\")\n",
    "        return\n",
    "    \n",
    "    # Get valid uncertainty data\n",
    "    required_cols = ['x', 'y', layer_name] + [col for col in uncertainty_cols.values() if col in df.columns]\n",
    "    valid_data = df.dropna(subset=required_cols)\n",
    "    \n",
    "    if len(valid_data) < n_locations:\n",
    "        print(f\"Not enough valid uncertainty data for {layer_name}\")\n",
    "        n_locations = len(valid_data)\n",
    "    \n",
    "    # Sort by log uncertainty (highest first) - most important for ModEx\n",
    "    optimal_locations = valid_data.nlargest(n_locations, uncertainty_cols['log'])\n",
    "    \n",
    "    # Prepare output columns\n",
    "    output_cols = ['x', 'y', layer_name] + [col for col in uncertainty_cols.values() if col in optimal_locations.columns]\n",
    "    \n",
    "    # Add ranking and priority classification\n",
    "    optimal_locations = optimal_locations.copy()\n",
    "    optimal_locations['uncertainty_rank'] = range(1, len(optimal_locations) + 1)\n",
    "    optimal_locations['modex_priority'] = pd.cut(\n",
    "        optimal_locations['uncertainty_rank'], \n",
    "        bins=[0, 5, 10, n_locations], \n",
    "        labels=['HIGH', 'MEDIUM', 'LOW']\n",
    "    )\n",
    "    \n",
    "    # Calculate expected information gain (simplified)\n",
    "    if uncertainty_cols['log'] in optimal_locations.columns:\n",
    "        optimal_locations['expected_info_gain'] = optimal_locations[uncertainty_cols['log']]**2\n",
    "    \n",
    "    # Save to CSV\n",
    "    output_cols.extend(['uncertainty_rank', 'modex_priority', 'expected_info_gain'])\n",
    "    optimal_locations[output_cols].to_csv(\n",
    "        f\"{layer_name}_optimal_survey_locations_comprehensive.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Saved top {len(optimal_locations)} optimal survey locations for {layer_name}\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    if uncertainty_cols['log'] in optimal_locations.columns:\n",
    "        print(f\"  Mean log uncertainty at optimal locations: {optimal_locations[uncertainty_cols['log']].mean():.4f}\")\n",
    "        print(f\"  Mean log uncertainty overall: {df[uncertainty_cols['log']].mean():.4f}\")\n",
    "    \n",
    "    if uncertainty_cols['cv'] in optimal_locations.columns:\n",
    "        print(f\"  Mean CV% at optimal locations: {optimal_locations[uncertainty_cols['cv']].mean():.1f}%\")\n",
    "        print(f\"  Mean CV% overall: {df[uncertainty_cols['cv']].mean():.1f}%\")\n",
    "    \n",
    "    return optimal_locations\n",
    "\n",
    "# Function to combine high uncertainty locations across ALL layers\n",
    "def save_combined_optimal_survey_locations(df, depth_layers, n_locations=50):\n",
    "    \"\"\"\n",
    "    Identify optimal survey locations considering high uncertainty across ALL layers.\n",
    "    This creates a single survey plan for field campaigns.\n",
    "    Uses ORIGINAL SCALE uncertainties (Ω·m units).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"COMBINING HIGH UNCERTAINTY LOCATIONS ACROSS ALL LAYERS\")\n",
    "    print(\"(Using original scale uncertainties in Ω·m)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Collect uncertainty data from all layers\n",
    "    uncertainty_data = []\n",
    "    \n",
    "    for layer in depth_layers:\n",
    "        # Use original scale uncertainty measures\n",
    "        uncertainty_std_col = f\"{layer}_uncertainty_std\"  # Standard deviation in Ω·m\n",
    "        uncertainty_linear_col = f\"{layer}_uncertainty_linear\"  # Linear approximation in Ω·m\n",
    "        cv_col = f\"{layer}_cv_percent\"  # CV% (dimensionless)\n",
    "        log_ref_col = f\"{layer}_log_uncertainty_ref\"  # Log reference for priority scoring\n",
    "        \n",
    "        # Check which uncertainty columns are available\n",
    "        available_cols = [col for col in [uncertainty_std_col, uncertainty_linear_col, cv_col, log_ref_col] if col in df.columns]\n",
    "        \n",
    "        if available_cols:\n",
    "            # Get locations with uncertainty data for this layer\n",
    "            required_cols = ['x', 'y', layer] + available_cols\n",
    "            layer_data = df[required_cols].dropna()\n",
    "            \n",
    "            if len(layer_data) > 0:\n",
    "                layer_data['layer_name'] = layer\n",
    "                # Use the most appropriate uncertainty measure for prioritization\n",
    "                if uncertainty_std_col in layer_data.columns:\n",
    "                    layer_data['uncertainty_for_priority'] = layer_data[uncertainty_std_col]\n",
    "                    layer_data['uncertainty_type'] = 'std_dev_ohm_m'\n",
    "                elif uncertainty_linear_col in layer_data.columns:\n",
    "                    layer_data['uncertainty_for_priority'] = layer_data[uncertainty_linear_col]\n",
    "                    layer_data['uncertainty_type'] = 'linear_approx_ohm_m'\n",
    "                elif cv_col in layer_data.columns:\n",
    "                    layer_data['uncertainty_for_priority'] = layer_data[cv_col]\n",
    "                    layer_data['uncertainty_type'] = 'cv_percent'\n",
    "                \n",
    "                uncertainty_data.append(layer_data)\n",
    "    \n",
    "    if not uncertainty_data:\n",
    "        print(\"No uncertainty data found across any layers!\")\n",
    "        return None\n",
    "    \n",
    "    # Combine all uncertainty data\n",
    "    all_uncertainty = pd.concat(uncertainty_data, ignore_index=True)\n",
    "    print(f\"Total uncertainty measurements across all layers: {len(all_uncertainty):,}\")\n",
    "    \n",
    "    # For each location (x, y), calculate combined uncertainty metrics\n",
    "    location_groups = all_uncertainty.groupby(['x', 'y'])\n",
    "    \n",
    "    combined_locations = []\n",
    "    for (x, y), group in location_groups:\n",
    "        # Calculate combined uncertainty metrics IN ORIGINAL SCALE\n",
    "        max_uncertainty = group['uncertainty_for_priority'].max()  # Max uncertainty across layers\n",
    "        mean_uncertainty = group['uncertainty_for_priority'].mean()  # Mean uncertainty across layers\n",
    "        n_layers_with_data = len(group)  # Number of layers with uncertainty data\n",
    "        \n",
    "        # Get uncertainty statistics by type\n",
    "        cv_data = group[group['uncertainty_type'] == 'cv_percent']['uncertainty_for_priority']\n",
    "        std_data = group[group['uncertainty_type'].str.contains('ohm_m', na=False)]['uncertainty_for_priority']\n",
    "        \n",
    "        max_cv = cv_data.max() if len(cv_data) > 0 else np.nan\n",
    "        mean_cv = cv_data.mean() if len(cv_data) > 0 else np.nan\n",
    "        max_std_ohm_m = std_data.max() if len(std_data) > 0 else np.nan\n",
    "        mean_std_ohm_m = std_data.mean() if len(std_data) > 0 else np.nan\n",
    "        \n",
    "        # Calculate combined priority score \n",
    "        # Use log reference if available for priority scoring (more stable)\n",
    "        log_ref_data = []\n",
    "        for layer in group['layer_name'].unique():\n",
    "            log_ref_col = f\"{layer}_log_uncertainty_ref\"\n",
    "            if log_ref_col in df.columns:\n",
    "                log_val = df[(df['x'] == x) & (df['y'] == y)][log_ref_col].dropna()\n",
    "                if len(log_val) > 0:\n",
    "                    log_ref_data.extend(log_val.values)\n",
    "        \n",
    "        if log_ref_data:\n",
    "            # Use log uncertainty for priority scoring (more stable)\n",
    "            max_log_uncertainty = max(log_ref_data)\n",
    "            mean_log_uncertainty = np.mean(log_ref_data)\n",
    "            priority_score = max_log_uncertainty * 0.7 + mean_log_uncertainty * 0.3\n",
    "        else:\n",
    "            # Fallback to normalized original scale uncertainty\n",
    "            normalized_uncertainty = max_uncertainty / (mean_uncertainty + 1e-10)  # Avoid division by zero\n",
    "            priority_score = normalized_uncertainty\n",
    "        \n",
    "        priority_score *= np.sqrt(n_layers_with_data)  # Bonus for locations with more layers\n",
    "        \n",
    "        # Get layers with high uncertainty at this location\n",
    "        high_uncertainty_threshold = group['uncertainty_for_priority'].quantile(0.7)\n",
    "        high_unc_layers = group[group['uncertainty_for_priority'] >= high_uncertainty_threshold]['layer_name'].tolist()\n",
    "        \n",
    "        combined_locations.append({\n",
    "            'x': x,\n",
    "            'y': y,\n",
    "            'max_uncertainty_original': max_uncertainty,\n",
    "            'mean_uncertainty_original': mean_uncertainty,\n",
    "            'max_cv_percent': max_cv,\n",
    "            'mean_cv_percent': mean_cv,\n",
    "            'max_std_ohm_m': max_std_ohm_m,\n",
    "            'mean_std_ohm_m': mean_std_ohm_m,\n",
    "            'n_layers_with_data': n_layers_with_data,\n",
    "            'priority_score': priority_score,\n",
    "            'high_uncertainty_layers': ', '.join(high_unc_layers),\n",
    "            'n_high_uncertainty_layers': len(high_unc_layers)\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    combined_df = pd.DataFrame(combined_locations)\n",
    "    print(f\"Unique survey locations identified: {len(combined_df):,}\")\n",
    "    \n",
    "    # Sort by priority score (highest first)\n",
    "    combined_df = combined_df.sort_values('priority_score', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Limit to requested number of locations\n",
    "    if len(combined_df) > n_locations:\n",
    "        optimal_df = combined_df.head(n_locations).copy()\n",
    "        print(f\"Selected top {n_locations} locations for survey campaign\")\n",
    "    else:\n",
    "        optimal_df = combined_df.copy()\n",
    "        print(f\"All {len(optimal_df)} locations recommended for survey\")\n",
    "    \n",
    "    # Add ranking and priority classification\n",
    "    optimal_df['survey_rank'] = range(1, len(optimal_df) + 1)\n",
    "    \n",
    "    # Classify priority based on rank percentiles\n",
    "    n_total = len(optimal_df)\n",
    "    optimal_df['modex_priority'] = 'LOW'\n",
    "    optimal_df.loc[:int(n_total*0.3), 'modex_priority'] = 'HIGH'\n",
    "    optimal_df.loc[int(n_total*0.3):int(n_total*0.6), 'modex_priority'] = 'MEDIUM'\n",
    "    \n",
    "    # Add expected information gain\n",
    "    optimal_df['expected_info_gain'] = optimal_df['priority_score']**2\n",
    "    \n",
    "    # Add practical survey recommendations\n",
    "    optimal_df['recommended_survey_type'] = 'Standard borehole + sampling'\n",
    "    optimal_df.loc[optimal_df['modex_priority'] == 'HIGH', 'recommended_survey_type'] = 'Deep borehole + well installation + comprehensive sampling'\n",
    "    optimal_df.loc[optimal_df['n_high_uncertainty_layers'] >= 3, 'recommended_survey_type'] = 'Multi-depth monitoring well + intensive sampling'\n",
    "    \n",
    "    # Calculate survey campaign phases\n",
    "    optimal_df['survey_phase'] = 'Phase 3'\n",
    "    optimal_df.loc[:int(n_total*0.2), 'survey_phase'] = 'Phase 1 (Immediate)'\n",
    "    optimal_df.loc[int(n_total*0.2):int(n_total*0.5), 'survey_phase'] = 'Phase 2 (Near-term)'\n",
    "    \n",
    "    # Save comprehensive survey plan\n",
    "    output_file = \"COMBINED_optimal_survey_locations_ALL_LAYERS.csv\"\n",
    "    optimal_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nSaved combined survey plan to: {output_file}\")\n",
    "    \n",
    "    # Print summary statistics (ALL IN ORIGINAL SCALE)\n",
    "    print(f\"\\nCOMBINED SURVEY PLAN SUMMARY (Original Scale Units):\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Total survey locations: {len(optimal_df)}\")\n",
    "    print(f\"Phase 1 (Immediate): {sum(optimal_df['survey_phase'] == 'Phase 1 (Immediate)')}\")\n",
    "    print(f\"Phase 2 (Near-term): {sum(optimal_df['survey_phase'] == 'Phase 2 (Near-term)')}\")\n",
    "    print(f\"Phase 3 (Future): {sum(optimal_df['survey_phase'] == 'Phase 3')}\")\n",
    "    \n",
    "    print(f\"\\nPriority breakdown:\")\n",
    "    print(f\"HIGH priority: {sum(optimal_df['modex_priority'] == 'HIGH')}\")\n",
    "    print(f\"MEDIUM priority: {sum(optimal_df['modex_priority'] == 'MEDIUM')}\")\n",
    "    print(f\"LOW priority: {sum(optimal_df['modex_priority'] == 'LOW')}\")\n",
    "    \n",
    "    print(f\"\\nUncertainty statistics (Original Scale):\")\n",
    "    if not optimal_df['max_std_ohm_m'].isna().all():\n",
    "        print(f\"Max std dev (Ω·m) range: {optimal_df['max_std_ohm_m'].min():.1f} - {optimal_df['max_std_ohm_m'].max():.1f}\")\n",
    "    if not optimal_df['max_cv_percent'].isna().all():\n",
    "        print(f\"Max CV% range: {optimal_df['max_cv_percent'].min():.1f}% - {optimal_df['max_cv_percent'].max():.1f}%\")\n",
    "    print(f\"Average layers per location: {optimal_df['n_layers_with_data'].mean():.1f}\")\n",
    "    \n",
    "    # Create summary by priority\n",
    "    numeric_cols = optimal_df.select_dtypes(include=[np.number]).columns\n",
    "    priority_summary = optimal_df.groupby('modex_priority')[numeric_cols].agg(['mean', 'std']).round(3)\n",
    "    \n",
    "    print(f\"\\nPriority-based summary:\")\n",
    "    print(priority_summary)\n",
    "    \n",
    "    # Save priority summary\n",
    "    priority_summary.to_csv(\"survey_priority_summary.csv\")\n",
    "    \n",
    "    return optimal_df\n",
    "\n",
    "# Function to create combined spatial visualization\n",
    "def plot_combined_survey_locations(df, survey_plan, depth_layers):\n",
    "    \"\"\"Create a spatial plot showing combined survey locations across all layers (ORIGINAL SCALE DATA).\"\"\"\n",
    "    \n",
    "    if survey_plan is None or len(survey_plan) == 0:\n",
    "        print(\"No survey plan data to plot\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Plot 1: All uncertainty data with survey locations\n",
    "    ax1 = axes[0]\n",
    "    \n",
    "    # Plot base uncertainty (use first layer as background with original scale uncertainty)\n",
    "    first_layer = depth_layers[0]\n",
    "    uncertainty_background_col = None\n",
    "    \n",
    "    # Try different uncertainty columns (prioritize original scale)\n",
    "    for col_suffix in ['_uncertainty_std', '_uncertainty_linear', '_cv_percent']:\n",
    "        test_col = f\"{first_layer}{col_suffix}\"\n",
    "        if test_col in df.columns:\n",
    "            uncertainty_background_col = test_col\n",
    "            break\n",
    "    \n",
    "    if uncertainty_background_col:\n",
    "        valid_mask = ~df[uncertainty_background_col].isna()\n",
    "        if valid_mask.sum() > 0:\n",
    "            scatter = ax1.scatter(df.loc[valid_mask, 'x'], \n",
    "                                df.loc[valid_mask, 'y'], \n",
    "                                c=df.loc[valid_mask, uncertainty_background_col], \n",
    "                                cmap='Greys', alpha=0.3, s=10, \n",
    "                                label='Background uncertainty')\n",
    "    \n",
    "    # Overlay survey locations by priority\n",
    "    high_priority = survey_plan[survey_plan['modex_priority'] == 'HIGH']\n",
    "    medium_priority = survey_plan[survey_plan['modex_priority'] == 'MEDIUM']\n",
    "    low_priority = survey_plan[survey_plan['modex_priority'] == 'LOW']\n",
    "    \n",
    "    if len(high_priority) > 0:\n",
    "        ax1.scatter(high_priority['x'], high_priority['y'], \n",
    "                   c='red', marker='X', s=200, alpha=0.9, \n",
    "                   label=f'HIGH priority (n={len(high_priority)})', \n",
    "                   edgecolors='black', linewidth=1)\n",
    "    \n",
    "    if len(medium_priority) > 0:\n",
    "        ax1.scatter(medium_priority['x'], medium_priority['y'], \n",
    "                   c='orange', marker='s', s=120, alpha=0.8, \n",
    "                   label=f'MEDIUM priority (n={len(medium_priority)})', \n",
    "                   edgecolors='black', linewidth=1)\n",
    "    \n",
    "    if len(low_priority) > 0:\n",
    "        ax1.scatter(low_priority['x'], low_priority['y'], \n",
    "                   c='yellow', marker='o', s=80, alpha=0.7, \n",
    "                   label=f'LOW priority (n={len(low_priority)})', \n",
    "                   edgecolors='black', linewidth=1)\n",
    "    \n",
    "    ax1.set_xlabel('X Coordinate')\n",
    "    ax1.set_ylabel('Y Coordinate')\n",
    "    ax1.set_title('Combined Survey Plan - All Priorities')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Survey phases\n",
    "    ax2 = axes[1]\n",
    "    \n",
    "    phase1 = survey_plan[survey_plan['survey_phase'] == 'Phase 1 (Immediate)']\n",
    "    phase2 = survey_plan[survey_plan['survey_phase'] == 'Phase 2 (Near-term)']\n",
    "    phase3 = survey_plan[survey_plan['survey_phase'] == 'Phase 3']\n",
    "    \n",
    "    # Base uncertainty background\n",
    "    if uncertainty_background_col and valid_mask.sum() > 0:\n",
    "        ax2.scatter(df.loc[valid_mask, 'x'], df.loc[valid_mask, 'y'], \n",
    "                   c=df.loc[valid_mask, uncertainty_background_col], \n",
    "                   cmap='Greys', alpha=0.3, s=10)\n",
    "    \n",
    "    if len(phase1) > 0:\n",
    "        ax2.scatter(phase1['x'], phase1['y'], \n",
    "                   c='darkred', marker='D', s=150, alpha=0.9, \n",
    "                   label=f'Phase 1 - Immediate (n={len(phase1)})', \n",
    "                   edgecolors='white', linewidth=2)\n",
    "    \n",
    "    if len(phase2) > 0:\n",
    "        ax2.scatter(phase2['x'], phase2['y'], \n",
    "                   c='darkorange', marker='s', s=100, alpha=0.8, \n",
    "                   label=f'Phase 2 - Near-term (n={len(phase2)})', \n",
    "                   edgecolors='white', linewidth=1)\n",
    "    \n",
    "    if len(phase3) > 0:\n",
    "        ax2.scatter(phase3['x'], phase3['y'], \n",
    "                   c='darkgoldenrod', marker='o', s=60, alpha=0.7, \n",
    "                   label=f'Phase 3 - Future (n={len(phase3)})', \n",
    "                   edgecolors='white', linewidth=1)\n",
    "    \n",
    "    ax2.set_xlabel('X Coordinate')\n",
    "    ax2.set_ylabel('Y Coordinate')\n",
    "    ax2.set_title('Survey Campaign Phases')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Number of high uncertainty layers per location\n",
    "    ax3 = axes[2]\n",
    "    \n",
    "    scatter3 = ax3.scatter(survey_plan['x'], survey_plan['y'], \n",
    "                          c=survey_plan['n_high_uncertainty_layers'], \n",
    "                          cmap='plasma', s=100, alpha=0.8,\n",
    "                          edgecolors='black', linewidth=0.5)\n",
    "    plt.colorbar(scatter3, ax=ax3, label='Number of High Uncertainty Layers')\n",
    "    ax3.set_xlabel('X Coordinate')\n",
    "    ax3.set_ylabel('Y Coordinate')\n",
    "    ax3.set_title('Multi-Layer Uncertainty Hotspots')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Uncertainty magnitude (use original scale)\n",
    "    ax4 = axes[3]\n",
    "    \n",
    "    # Use the most appropriate uncertainty measure for visualization\n",
    "    uncertainty_for_viz = None\n",
    "    uncertainty_label = \"\"\n",
    "    \n",
    "    if 'max_std_ohm_m' in survey_plan.columns and not survey_plan['max_std_ohm_m'].isna().all():\n",
    "        uncertainty_for_viz = survey_plan['max_std_ohm_m']\n",
    "        uncertainty_label = 'Max Uncertainty Std Dev (Ω·m)'\n",
    "    elif 'max_cv_percent' in survey_plan.columns and not survey_plan['max_cv_percent'].isna().all():\n",
    "        uncertainty_for_viz = survey_plan['max_cv_percent']\n",
    "        uncertainty_label = 'Max Coefficient of Variation (%)'\n",
    "    elif 'max_uncertainty_original' in survey_plan.columns:\n",
    "        uncertainty_for_viz = survey_plan['max_uncertainty_original']\n",
    "        uncertainty_label = 'Max Uncertainty (Original Scale)'\n",
    "    \n",
    "    if uncertainty_for_viz is not None:\n",
    "        scatter4 = ax4.scatter(survey_plan['x'], survey_plan['y'], \n",
    "                              c=uncertainty_for_viz, \n",
    "                              cmap='Reds', s=100, alpha=0.8,\n",
    "                              edgecolors='black', linewidth=0.5)\n",
    "        plt.colorbar(scatter4, ax=ax4, label=uncertainty_label)\n",
    "        ax4.set_xlabel('X Coordinate')\n",
    "        ax4.set_ylabel('Y Coordinate')\n",
    "        ax4.set_title('Survey Uncertainty Magnitude (Original Scale)')\n",
    "    else:\n",
    "        # Fallback to priority score\n",
    "        scatter4 = ax4.scatter(survey_plan['x'], survey_plan['y'], \n",
    "                              c=survey_plan['priority_score'], \n",
    "                              cmap='Reds', s=100, alpha=0.8,\n",
    "                              edgecolors='black', linewidth=0.5)\n",
    "        plt.colorbar(scatter4, ax=ax4, label='Priority Score')\n",
    "        ax4.set_xlabel('X Coordinate')\n",
    "        ax4.set_ylabel('Y Coordinate')\n",
    "        ax4.set_title('Survey Priority Scores')\n",
    "    \n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('COMBINED_survey_plan_spatial_analysis.png', \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Saved combined survey plan visualization to: COMBINED_survey_plan_spatial_analysis.png\")\n",
    "\n",
    "# Function to create uncertainty interpretation guide\n",
    "def create_uncertainty_interpretation_guide(layer_name, sample_data):\n",
    "    \"\"\"Create a guide for interpreting uncertainty values (ALL IN ORIGINAL SCALE).\"\"\"\n",
    "    \n",
    "    guide_text = f\"\"\"\n",
    "UNCERTAINTY INTERPRETATION GUIDE FOR {layer_name.upper()}\n",
    "========================================================\n",
    "*** ALL UNCERTAINTY MEASURES IN ORIGINAL SCALE (Ω·m) OR DIMENSIONLESS ***\n",
    "\n",
    "UNITS AND MEANINGS:\n",
    "------------------\n",
    "1. {layer_name}_uncertainty_std [Ω·m]:\n",
    "   - Standard deviation of resistivity in original units\n",
    "   - Interpretation: ±1σ contains ~68% of probable values\n",
    "   - Example: 50 Ω·m means prediction ±50 Ω·m\n",
    "   - Use: Most statistically rigorous uncertainty measure\n",
    "\n",
    "2. {layer_name}_uncertainty_linear [Ω·m]:\n",
    "   - Linear approximation of uncertainty in original units\n",
    "   - Interpretation: Simpler approximation for small uncertainties\n",
    "   - Example: 30 Ω·m means prediction ±30 Ω·m (approximately)\n",
    "   - Use: Easier calculations, good for small uncertainties\n",
    "\n",
    "3. {layer_name}_uncertainty_range [Ω·m]:\n",
    "   - Half-width of 95% confidence interval in original units\n",
    "   - Interpretation: 95% CI = prediction ± range\n",
    "   - Example: 80 Ω·m means 95% CI spans ±80 Ω·m\n",
    "   - Use: Direct confidence interval interpretation\n",
    "\n",
    "4. {layer_name}_uncertainty_factor [dimensionless]:\n",
    "   - Multiplicative uncertainty factor\n",
    "   - Interpretation: Prediction could be X× higher or lower\n",
    "   - Example: 1.5 means prediction could be 1.5× higher or lower\n",
    "   - Use: Relative uncertainty (independent of magnitude)\n",
    "\n",
    "5. {layer_name}_cv_percent [%]:\n",
    "   - Coefficient of variation as percentage\n",
    "   - Interpretation: Relative uncertainty as percentage\n",
    "   - Example: 25% means prediction ±25% relative uncertainty\n",
    "   - Use: Most intuitive for field teams and managers\n",
    "\n",
    "6. {layer_name}_ci_lower, {layer_name}_ci_upper [Ω·m]:\n",
    "   - 95% confidence interval bounds in original units\n",
    "   - Interpretation: 95% chance true value lies in this range\n",
    "   - Example: [150, 350] means true resistivity likely 150-350 Ω·m\n",
    "   - Use: Most useful for engineering decisions and risk assessment\n",
    "\n",
    "MODEX SURVEY PRIORITIZATION (ORIGINAL SCALE BASED):\n",
    "--------------------------------------------------\"\"\"\n",
    "    \n",
    "    # Add dynamic thresholds based on sample data\n",
    "    if 'uncertainty_std' in sample_data and len(sample_data['uncertainty_std']) > 0:\n",
    "        std_90th = np.percentile(sample_data['uncertainty_std'], 90)\n",
    "        std_80th = np.percentile(sample_data['uncertainty_std'], 80)\n",
    "        guide_text += f\"\"\"\n",
    "- HIGH priority: uncertainty_std > {std_90th:.0f} Ω·m or CV > {np.percentile(sample_data.get('cv_percent', [0]), 90):.0f}%\n",
    "- MEDIUM priority: uncertainty_std > {std_80th:.0f} Ω·m or CV > {np.percentile(sample_data.get('cv_percent', [0]), 80):.0f}%\n",
    "- LOW priority: uncertainty_std < {std_80th:.0f} Ω·m and CV < {np.percentile(sample_data.get('cv_percent', [0]), 80):.0f}%\"\"\"\n",
    "    else:\n",
    "        guide_text += f\"\"\"\n",
    "- HIGH priority: CV > {np.percentile(sample_data.get('cv_percent', [0]), 90):.0f}%\n",
    "- MEDIUM priority: CV > {np.percentile(sample_data.get('cv_percent', [0]), 80):.0f}%\n",
    "- LOW priority: CV < {np.percentile(sample_data.get('cv_percent', [0]), 80):.0f}%\"\"\"\n",
    "    \n",
    "    guide_text += f\"\"\"\n",
    "\n",
    "FIELD CAMPAIGN RECOMMENDATIONS:\n",
    "------------------------------\n",
    "- Deploy boreholes/wells at HIGH priority locations first\n",
    "- Use dense soil sampling in MEDIUM priority areas\n",
    "- Standard monitoring sufficient for LOW priority areas\n",
    "- Expected uncertainty reduction: 40-70% per new measurement\n",
    "- Focus on locations with uncertainty_std > {np.percentile(sample_data.get('uncertainty_std', [0]), 85):.0f} Ω·m\n",
    "\n",
    "PRACTICAL EXAMPLES FOR {layer_name.upper()}:\n",
    "-------------------------------------------\"\"\"\n",
    "    \n",
    "    if len(sample_data.get('predictions', [])) > 0:\n",
    "        sample_pred = np.percentile(sample_data['predictions'], 50)  # Median prediction\n",
    "        sample_std = np.percentile(sample_data.get('uncertainty_std', [0]), 50)  # Median uncertainty\n",
    "        sample_cv = np.percentile(sample_data.get('cv_percent', [0]), 50)  # Median CV\n",
    "        \n",
    "        guide_text += f\"\"\"\n",
    "Example location with median uncertainty:\n",
    "- Predicted resistivity: {sample_pred:.0f} Ω·m\n",
    "- Uncertainty (±1σ): {sample_std:.0f} Ω·m\n",
    "- Coefficient of variation: {sample_cv:.0f}%\n",
    "- 68% confidence range: {sample_pred-sample_std:.0f} - {sample_pred+sample_std:.0f} Ω·m\n",
    "- 95% confidence range: {sample_pred-1.96*sample_std:.0f} - {sample_pred+1.96*sample_std:.0f} Ω·m\"\"\"\n",
    "\n",
    "    guide_text += f\"\"\"\n",
    "\n",
    "THEORETICAL BASIS:\n",
    "-----------------\n",
    "- Random Forest ensemble variance: Reflects model epistemic uncertainty\n",
    "- Spatial correlation modeling: Accounts for geostatistical structure  \n",
    "- Bootstrap confidence intervals: Provides calibrated uncertainty bounds\n",
    "- Log-normal distribution: Appropriate for resistivity data\n",
    "- Information theory: Prioritizes maximum expected information gain\n",
    "\n",
    "QUALITY ASSURANCE:\n",
    "-----------------\n",
    "- All uncertainties converted from log-space to original Ω·m units\n",
    "- Confidence intervals calibrated to maintain ~95% coverage\n",
    "- Multiple uncertainty representations for different use cases\n",
    "- Spatial uncertainty accounts for distance to training data\n",
    "- Uncertainty validated against out-of-bag samples\n",
    "\n",
    "INTEGRATION WITH MODEX FRAMEWORK:\n",
    "--------------------------------\n",
    "- Phase 5: Use HIGH priority locations for immediate field campaigns\n",
    "- Phase 6: Validate predictions against new measurements at survey sites\n",
    "- Iteration: Retrain model with new data to reduce uncertainty\n",
    "- Target: Achieve <20% CV for critical engineering decisions\n",
    "- Monitoring: Ensure prediction intervals maintain proper coverage\n",
    "\"\"\"\n",
    "    \n",
    "    # Save guide to file with UTF-8 encoding to handle special characters\n",
    "    with open(f\"{layer_name}_uncertainty_interpretation_guide.txt\", 'w', encoding='utf-8') as f:\n",
    "        f.write(guide_text)\n",
    "    \n",
    "    print(f\"Saved uncertainty interpretation guide to {layer_name}_uncertainty_interpretation_guide.txt\")\n",
    "\n",
    "# Main function to train model and predict for a specific depth layer with comprehensive uncertainty\n",
    "def train_and_predict_for_layer_comprehensive(df, layer_name, feature_columns):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROCESSING {layer_name.upper()} WITH COMPREHENSIVE UNCERTAINTY ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Split data into known and unknown sets\n",
    "    known_data = df.dropna(subset=[layer_name]).copy()\n",
    "    unknown_data = df[df[layer_name].isna()].copy()\n",
    "    \n",
    "    print(f\"Training data size: {known_data.shape[0]} rows\")\n",
    "    print(f\"Prediction data size: {unknown_data.shape[0]} rows\")\n",
    "    \n",
    "    if known_data.shape[0] < 10:\n",
    "        print(f\"WARNING: Not enough training data for {layer_name}. Skipping.\")\n",
    "        return df\n",
    "    \n",
    "    # Further filter known data to only include rows where all features are non-NaN\n",
    "    known_data = known_data.dropna(subset=feature_columns)\n",
    "    print(f\"Training data size after removing NaN features: {known_data.shape[0]} rows\")\n",
    "    \n",
    "    if known_data.shape[0] < 10:\n",
    "        print(f\"WARNING: Not enough non-NaN training data for {layer_name}. Skipping.\")\n",
    "        return df\n",
    "    \n",
    "    # Extract features and target\n",
    "    X = known_data[feature_columns]\n",
    "    y = known_data[layer_name]\n",
    "    \n",
    "    # First apply log10 transformation to the target variable\n",
    "    print(f\"Applying log10 transformation to {layer_name}\")\n",
    "    y_log, log_offset = safe_log10_transform(y)\n",
    "    \n",
    "    # Remove NaN values before scaling\n",
    "    valid_mask = ~np.isnan(y_log)\n",
    "    if not np.any(valid_mask):\n",
    "        print(f\"WARNING: All values became NaN after log transform for {layer_name}. Skipping.\")\n",
    "        return df\n",
    "    \n",
    "    # Extract valid data points\n",
    "    X_valid = X[valid_mask]\n",
    "    y_log_valid = y_log[valid_mask]\n",
    "    \n",
    "    # EXPLICITLY normalize input features to 0-1 range\n",
    "    feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_normalized = feature_scaler.fit_transform(X_valid)\n",
    "    \n",
    "    # Print before and after normalization stats\n",
    "    print(\"\\nInput features before normalization:\")\n",
    "    for i, col in enumerate(feature_columns):\n",
    "        col_values = X_valid[col].values\n",
    "        print(f\"  {col}: min={np.min(col_values):.4f}, max={np.max(col_values):.4f}, mean={np.mean(col_values):.4f}\")\n",
    "    \n",
    "    print(\"\\nInput features after normalization:\")\n",
    "    for i, col in enumerate(feature_columns):\n",
    "        col_values = X_normalized[:, i]\n",
    "        print(f\"  {col}: min={np.min(col_values):.4f}, max={np.max(col_values):.4f}, mean={np.mean(col_values):.4f}\")\n",
    "    \n",
    "    # Create output scaler for the log-transformed target variable (0-1 range)\n",
    "    output_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # Properly reshape to 2D array for scaling\n",
    "    y_log_valid_2d = y_log_valid.values.reshape(-1, 1)\n",
    "    y_normalized_valid = output_scaler.fit_transform(y_log_valid_2d).flatten()\n",
    "    \n",
    "    # Split the data with valid values only\n",
    "    X_train, X_test, y_train_norm, y_test_norm = train_test_split(\n",
    "        X_normalized, y_normalized_valid, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Also keep track of original values for evaluation\n",
    "    _, _, y_train_orig, y_test_orig = train_test_split(\n",
    "        X_valid, y[valid_mask], test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the RandomForest model with uncertainty estimation\n",
    "    print(\"\\nTraining Random Forest with comprehensive uncertainty estimation...\")\n",
    "    model = RandomForestWithUncertainty(\n",
    "        n_estimators=100, \n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train_norm)\n",
    "    \n",
    "    # Store scalers and transformation info in model\n",
    "    model.feature_scaler = feature_scaler\n",
    "    model.output_scaler = output_scaler\n",
    "    model.log_offset = log_offset\n",
    "    \n",
    "    # Generate feature importance plot\n",
    "    plot_feature_importance(model, feature_columns, layer_name)\n",
    "    \n",
    "    # Evaluate on test set with uncertainty\n",
    "    print(\"\\nEvaluating model with uncertainty quantification...\")\n",
    "    y_pred_norm, y_uncertainty_norm, y_intervals_norm = model.predict_with_uncertainty(\n",
    "        X_test, method='combined'\n",
    "    )\n",
    "    \n",
    "    # Convert normalized predictions back to log scale\n",
    "    y_pred_log = output_scaler.inverse_transform(y_pred_norm.reshape(-1, 1)).flatten()\n",
    "    y_uncertainty_log = y_uncertainty_norm * (output_scaler.data_max_ - output_scaler.data_min_)\n",
    "    \n",
    "    # Convert to original scale\n",
    "    y_pred_orig = inverse_log10_transform(y_pred_log, log_offset)\n",
    "    \n",
    "    # Calculate metrics using original scale\n",
    "    mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
    "    r2 = r2_score(y_test_orig, y_pred_orig)\n",
    "    mean_uncertainty = np.mean(y_uncertainty_norm)\n",
    "    \n",
    "    print(f\"\\nModel Performance Metrics:\")\n",
    "    print(f\"  Mean Squared Error (original scale): {mse:.4f}\")\n",
    "    print(f\"  R² Score: {r2:.4f}\")\n",
    "    print(f\"  Mean Prediction Uncertainty (normalized): {mean_uncertainty:.4f}\")\n",
    "    print(f\"  Mean Log Uncertainty: {np.mean(y_uncertainty_log):.4f}\")\n",
    "    \n",
    "    # Calculate uncertainty calibration\n",
    "    y_intervals_log = output_scaler.inverse_transform(y_intervals_norm.reshape(-1, 2))\n",
    "    y_intervals_orig = np.column_stack([\n",
    "        inverse_log10_transform(y_intervals_log[:, 0], log_offset),\n",
    "        inverse_log10_transform(y_intervals_log[:, 1], log_offset)\n",
    "    ])\n",
    "    \n",
    "    coverage = np.mean((y_test_orig >= y_intervals_orig[:, 0]) & \n",
    "                      (y_test_orig <= y_intervals_orig[:, 1]))\n",
    "    print(f\"  Prediction Interval Coverage: {coverage:.2%} (target: 95%)\")\n",
    "    \n",
    "    # Predict unknown values with comprehensive uncertainty\n",
    "    if unknown_data.shape[0] > 0:\n",
    "        # Create a mask for rows with all non-NaN features\n",
    "        valid_features_mask = unknown_data[feature_columns].notna().all(axis=1)\n",
    "        print(f\"\\nFound {valid_features_mask.sum()} out of {unknown_data.shape[0]} unknown rows with all features non-NaN\")\n",
    "        \n",
    "        # Only make predictions for rows with all non-NaN features\n",
    "        if valid_features_mask.sum() > 0:\n",
    "            # Extract data for prediction\n",
    "            valid_unknown_data = unknown_data[valid_features_mask]\n",
    "            X_unknown = valid_unknown_data[feature_columns]\n",
    "            \n",
    "            # Normalize the unknown features using the same scaler\n",
    "            X_unknown_normalized = feature_scaler.transform(X_unknown)\n",
    "            \n",
    "            # Make predictions with uncertainty (in normalized space)\n",
    "            unknown_pred_norm, unknown_uncertainty_norm, unknown_intervals_norm = model.predict_with_uncertainty(\n",
    "                X_unknown_normalized, method='combined'\n",
    "            )\n",
    "            \n",
    "            # Convert predictions back to log scale\n",
    "            unknown_pred_log = output_scaler.inverse_transform(\n",
    "                unknown_pred_norm.reshape(-1, 1)\n",
    "            ).flatten()\n",
    "            unknown_uncertainty_log = unknown_uncertainty_norm * (output_scaler.data_max_ - output_scaler.data_min_)\n",
    "            \n",
    "            # Convert to original scale\n",
    "            unknown_predictions = inverse_log10_transform(unknown_pred_log, log_offset)\n",
    "            \n",
    "            # Convert uncertainty to all formats IN ORIGINAL SCALE\n",
    "            uncertainty_formats = convert_uncertainty_to_original_scale(\n",
    "                unknown_pred_log, unknown_uncertainty_log, log_offset\n",
    "            )\n",
    "            \n",
    "            # Create a copy of the original dataframe\n",
    "            updated_df = df.copy()\n",
    "            \n",
    "            # Update predictions and all uncertainty measures (ALL IN ORIGINAL SCALE Ω·m)\n",
    "            mask = updated_df[layer_name].isna() & updated_df[feature_columns].notna().all(axis=1)\n",
    "            \n",
    "            # Store predictions in Ω·m\n",
    "            updated_df.loc[mask, layer_name] = unknown_predictions\n",
    "            \n",
    "            # Store all uncertainty formats IN ORIGINAL SCALE\n",
    "            updated_df.loc[mask, f\"{layer_name}_uncertainty_std\"] = uncertainty_formats['uncertainty_std_ohm_m']  # Ω·m\n",
    "            updated_df.loc[mask, f\"{layer_name}_uncertainty_linear\"] = uncertainty_formats['uncertainty_linear_ohm_m']  # Ω·m (simpler)\n",
    "            updated_df.loc[mask, f\"{layer_name}_uncertainty_range\"] = uncertainty_formats['uncertainty_range_ohm_m']  # Ω·m\n",
    "            updated_df.loc[mask, f\"{layer_name}_uncertainty_factor\"] = uncertainty_formats['uncertainty_factor']  # dimensionless\n",
    "            updated_df.loc[mask, f\"{layer_name}_cv_percent\"] = uncertainty_formats['cv_percent']  # %\n",
    "            updated_df.loc[mask, f\"{layer_name}_ci_lower\"] = uncertainty_formats['ci_lower_ohm_m']  # Ω·m\n",
    "            updated_df.loc[mask, f\"{layer_name}_ci_upper\"] = uncertainty_formats['ci_upper_ohm_m']  # Ω·m\n",
    "            \n",
    "            # Keep log uncertainty for internal calculations only (marked as reference)\n",
    "            updated_df.loc[mask, f\"{layer_name}_log_uncertainty_ref\"] = uncertainty_formats['log_uncertainty_reference']\n",
    "            \n",
    "            print(f\"Filled {sum(mask)} missing values for {layer_name}\")\n",
    "            print(f\"Left {sum(updated_df[layer_name].isna())} values as NaN (either target or features had NaN)\")\n",
    "            \n",
    "            # Print uncertainty statistics IN ORIGINAL SCALE\n",
    "            print(f\"\\nUncertainty Statistics for {layer_name} (ALL IN ORIGINAL SCALE):\")\n",
    "            print(f\"  Resistivity predictions (Ω·m) - Mean: {unknown_predictions.mean():.1f}, Range: [{unknown_predictions.min():.1f}, {unknown_predictions.max():.1f}]\")\n",
    "            print(f\"  Uncertainty std dev (Ω·m) - Mean: {uncertainty_formats['uncertainty_std_ohm_m'].mean():.1f}, Max: {uncertainty_formats['uncertainty_std_ohm_m'].max():.1f}\")\n",
    "            print(f\"  Uncertainty linear approx (Ω·m) - Mean: {uncertainty_formats['uncertainty_linear_ohm_m'].mean():.1f}, Max: {uncertainty_formats['uncertainty_linear_ohm_m'].max():.1f}\")\n",
    "            print(f\"  CV% - Mean: {uncertainty_formats['cv_percent'].mean():.1f}%, Max: {uncertainty_formats['cv_percent'].max():.1f}%\")\n",
    "            print(f\"  Uncertainty factor - Mean: {uncertainty_formats['uncertainty_factor'].mean():.2f}×, Max: {uncertainty_formats['uncertainty_factor'].max():.2f}×\")\n",
    "            \n",
    "            # Generate comprehensive spatial analysis\n",
    "            plot_spatial_distribution_with_uncertainty(updated_df, layer_name)\n",
    "            \n",
    "            # Create uncertainty interpretation guide\n",
    "            sample_data = {\n",
    "                'predictions': unknown_predictions,\n",
    "                'uncertainty_std': uncertainty_formats['uncertainty_std_ohm_m'],\n",
    "                'uncertainty_linear': uncertainty_formats['uncertainty_linear_ohm_m'],\n",
    "                'cv_percent': uncertainty_formats['cv_percent']\n",
    "            }\n",
    "            create_uncertainty_interpretation_guide(layer_name, sample_data)\n",
    "            \n",
    "            print(f\"\\nProcessing time: {time.time() - start_time:.2f} seconds\")\n",
    "            return updated_df\n",
    "        else:\n",
    "            print(f\"No valid rows with all non-NaN features to predict for {layer_name}\")\n",
    "            return df\n",
    "    else:\n",
    "        print(f\"No missing values to predict for {layer_name}\")\n",
    "        return df\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    start_time = time.time()  # Track total processing time\n",
    "    \n",
    "    print(\"STARTING COMPREHENSIVE RESISTIVITY PREDICTION WITH UNCERTAINTY ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    if not os.path.exists('modex_output'):\n",
    "        os.makedirs('modex_output')\n",
    "        print(\"Created modex_output directory for results\")\n",
    "    \n",
    "    # Process each depth layer with comprehensive uncertainty estimation\n",
    "    updated_df = df.copy()\n",
    "    \n",
    "    for i, layer in enumerate(depth_layers):\n",
    "        print(f\"\\nProcessing layer {i+1}/{len(depth_layers)}: {layer}\")\n",
    "        updated_df = train_and_predict_for_layer_comprehensive(updated_df, layer, feature_columns)\n",
    "    \n",
    "    # Save the updated dataset with all predictions and uncertainty measures\n",
    "    output_file = \"terrain_with_comprehensive_resistivity_predictions.csv\"\n",
    "    updated_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\nSaved complete dataset with predictions and uncertainties to {output_file}\")\n",
    "    \n",
    "    # Generate combined optimal survey locations across ALL layers\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"GENERATING COMBINED SURVEY PLAN FOR ALL LAYERS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    combined_survey_plan = save_combined_optimal_survey_locations(\n",
    "        updated_df, depth_layers, n_locations=50\n",
    "    )\n",
    "    \n",
    "    # Create combined spatial visualization\n",
    "    if combined_survey_plan is not None:\n",
    "        plot_combined_survey_locations(updated_df, combined_survey_plan, depth_layers)\n",
    "    \n",
    "    # Generate comprehensive summary report\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"COMPREHENSIVE MODEX FRAMEWORK SUMMARY REPORT\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"  Total locations: {len(updated_df):,}\")\n",
    "    print(f\"  Depth layers processed: {len(depth_layers)}\")\n",
    "    print(f\"  Features used: {feature_columns}\")\n",
    "    print(f\"  Processing date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Summary for each layer\n",
    "    summary_data = []\n",
    "    for layer in depth_layers:\n",
    "        uncertainty_cols = {\n",
    "            'log': f\"{layer}_uncertainty_log\",\n",
    "            'cv': f\"{layer}_cv_percent\",\n",
    "            'factor': f\"{layer}_uncertainty_factor\"\n",
    "        }\n",
    "        \n",
    "        if uncertainty_cols['log'] in updated_df.columns:\n",
    "            print(f\"\\n{layer.upper()} - ModEx Analysis Summary:\")\n",
    "            \n",
    "            # Data coverage\n",
    "            predicted_count = updated_df[layer].notna().sum()\n",
    "            coverage_pct = predicted_count / len(updated_df) * 100\n",
    "            \n",
    "            # Uncertainty statistics\n",
    "            mean_log_unc = updated_df[uncertainty_cols['log']].mean()\n",
    "            mean_cv = updated_df[uncertainty_cols['cv']].mean()\n",
    "            max_cv = updated_df[uncertainty_cols['cv']].max()\n",
    "            \n",
    "            # High uncertainty locations\n",
    "            high_unc_90_count = (updated_df[uncertainty_cols['log']] >= updated_df[uncertainty_cols['log']].quantile(0.9)).sum()\n",
    "            high_unc_80_count = (updated_df[uncertainty_cols['log']] >= updated_df[uncertainty_cols['log']].quantile(0.8)).sum()\n",
    "            \n",
    "            print(f\"  Spatial coverage: {coverage_pct:.1f}% ({predicted_count:,} locations)\")\n",
    "            print(f\"  Mean uncertainty: {mean_log_unc:.3f} log10(Ω·m) = {mean_cv:.1f}% CV\")\n",
    "            print(f\"  Max uncertainty: {max_cv:.1f}% CV\")\n",
    "            print(f\"  High priority survey locations (top 10%): {high_unc_90_count}\")\n",
    "            print(f\"  Medium priority survey locations (top 20%): {high_unc_80_count}\")\n",
    "            \n",
    "            # Classification\n",
    "            if coverage_pct > 80 and mean_cv < 50:\n",
    "                confidence = \"HIGH\"\n",
    "            elif coverage_pct > 60 and mean_cv < 100:\n",
    "                confidence = \"MEDIUM\"\n",
    "            else:\n",
    "                confidence = \"LOW\"\n",
    "            \n",
    "            if high_unc_90_count > 50:\n",
    "                survey_priority = \"HIGH\"\n",
    "            elif high_unc_90_count > 20:\n",
    "                survey_priority = \"MEDIUM\"\n",
    "            else:\n",
    "                survey_priority = \"LOW\"\n",
    "            \n",
    "            print(f\"  Model confidence: {confidence}\")\n",
    "            print(f\"  Survey campaign priority: {survey_priority}\")\n",
    "            \n",
    "            # Store for summary table\n",
    "            summary_data.append({\n",
    "                'Layer': layer,\n",
    "                'Coverage_Pct': coverage_pct,\n",
    "                'Mean_CV_Pct': mean_cv,\n",
    "                'High_Priority_Locations': high_unc_90_count,\n",
    "                'Model_Confidence': confidence,\n",
    "                'Survey_Priority': survey_priority\n",
    "            })\n",
    "    \n",
    "    # Create summary table\n",
    "    if summary_data:\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        summary_df.to_csv(\"modex_summary_table.csv\", index=False)\n",
    "        print(f\"\\nSaved summary table to modex_summary_table.csv\")\n",
    "    \n",
    "    # ModEx cycle recommendations\n",
    "    print(f\"\\nNEXT STEPS FOR MODEX CYCLE:\")\n",
    "    print(f\"{'='*40}\")\n",
    "    print(f\"COMBINED SURVEY APPROACH:\")\n",
    "    if combined_survey_plan is not None:\n",
    "        phase1_count = sum(combined_survey_plan['survey_phase'] == 'Phase 1 (Immediate)')\n",
    "        phase2_count = sum(combined_survey_plan['survey_phase'] == 'Phase 2 (Near-term)')\n",
    "        high_priority_count = sum(combined_survey_plan['modex_priority'] == 'HIGH')\n",
    "        \n",
    "        print(f\"  Total survey locations identified: {len(combined_survey_plan)}\")\n",
    "        print(f\"  Phase 1 (Immediate deployment): {phase1_count} locations\")\n",
    "        print(f\"  Phase 2 (Near-term deployment): {phase2_count} locations\")\n",
    "        print(f\"  HIGH priority locations: {high_priority_count}\")\n",
    "        \n",
    "        print(f\"\\n  EFFICIENCY BENEFITS:\")\n",
    "        print(f\"  - Single borehole samples multiple depth layers\")\n",
    "        print(f\"  - Reduced field deployment costs vs. layer-by-layer approach\")\n",
    "        print(f\"  - Captures high uncertainty from ALL layers simultaneously\")\n",
    "        print(f\"  - Maximizes information gain per field site\")\n",
    "    \n",
    "    print(f\"\\nPhase 5 - Field Campaigns:\")\n",
    "    print(f\"  1. Review COMBINED_optimal_survey_locations_ALL_LAYERS.csv\")\n",
    "    print(f\"  2. Deploy Phase 1 locations FIRST (immediate impact)\")\n",
    "    print(f\"  3. Focus on HIGH priority locations for maximum uncertainty reduction\")\n",
    "    print(f\"  4. Sample ALL depth layers at each borehole location\")\n",
    "    print(f\"  5. Install monitoring wells at multi-layer uncertainty hotspots\")\n",
    "    \n",
    "    print(f\"\\nPhase 6 - Model Validation & Refinement:\")\n",
    "    print(f\"  1. Collect resistivity data across all layers at survey locations\")\n",
    "    print(f\"  2. Validate predictions against new measurements for each layer\")\n",
    "    print(f\"  3. Retrain models with expanded multi-layer dataset\")\n",
    "    print(f\"  4. Quantify uncertainty reduction achieved across all layers\")\n",
    "    \n",
    "    print(f\"\\nIterative Improvement:\")\n",
    "    print(f\"  - Expected uncertainty reduction: 40-80% per combined field campaign\")\n",
    "    print(f\"  - Target: Achieve <20% CV for critical areas across all layers\")\n",
    "    print(f\"  - Monitor: Prediction interval coverage remains ~95% for all layers\")\n",
    "    print(f\"  - Efficiency: ~3-5x cost reduction vs. layer-specific surveys\")\n",
    "    \n",
    "    print(f\"\\nFiles Generated:\")\n",
    "    print(f\"  MAIN SURVEY PLAN:\")\n",
    "    print(f\"    - COMBINED_optimal_survey_locations_ALL_LAYERS.csv (PRIMARY)\")\n",
    "    print(f\"    - COMBINED_survey_plan_spatial_analysis.png\")\n",
    "    print(f\"    - survey_priority_summary.csv\")\n",
    "    print(f\"  SUPPORTING ANALYSIS:\")\n",
    "    print(f\"    - {output_file} (complete predictions & uncertainties)\")\n",
    "    print(f\"    - *_comprehensive_spatial_analysis.png (individual layers)\")\n",
    "    print(f\"    - *_feature_importance.png\")\n",
    "    print(f\"    - *_uncertainty_interpretation_guide.txt\")\n",
    "    print(f\"    - modex_summary_table.csv\")\n",
    "    \n",
    "    print(f\"\\nCOMPREHENSIVE ANALYSIS COMPLETE!\")\n",
    "    print(f\"=\"*80)\n",
    "    \n",
    "    # CRITICAL: Explain saved column formats\n",
    "    print(f\"\\nSAVED DATA FORMAT EXPLANATION:\")\n",
    "    print(f\"=\"*40)\n",
    "    print(f\"Main CSV file: {output_file}\")\n",
    "    print(f\"\\nFor each layer (e.g., layer_1), the following columns are saved:\")\n",
    "    print(f\"  {depth_layers[0] if depth_layers else 'layer_X'}                    : Predicted resistivity [Ω·m]\")\n",
    "    print(f\"  {depth_layers[0] if depth_layers else 'layer_X'}_uncertainty_std   : Standard deviation uncertainty [Ω·m]\")\n",
    "    print(f\"  {depth_layers[0] if depth_layers else 'layer_X'}_uncertainty_linear: Linear approximation uncertainty [Ω·m]\")\n",
    "    print(f\"  {depth_layers[0] if depth_layers else 'layer_X'}_uncertainty_range : Half-width of 95% CI [Ω·m]\")\n",
    "    print(f\"  {depth_layers[0] if depth_layers else 'layer_X'}_uncertainty_factor: Multiplicative factor [dimensionless]\")\n",
    "    print(f\"  {depth_layers[0] if depth_layers else 'layer_X'}_cv_percent        : Coefficient of variation [%]\")\n",
    "    print(f\"  {depth_layers[0] if depth_layers else 'layer_X'}_ci_lower          : Lower 95% confidence bound [Ω·m]\")\n",
    "    print(f\"  {depth_layers[0] if depth_layers else 'layer_X'}_ci_upper          : Upper 95% confidence bound [Ω·m]\")\n",
    "    print(f\"  {depth_layers[0] if depth_layers else 'layer_X'}_log_uncertainty_ref: Log uncertainty [log10(Ω·m)] - for reference only\")\n",
    "    \n",
    "    print(f\"\\nRECOMMENDED USAGE:\")\n",
    "    print(f\"  - For plotting: Use '_uncertainty_std' or '_cv_percent' columns\")\n",
    "    print(f\"  - For field decisions: Use '_ci_lower' and '_ci_upper' columns\")\n",
    "    print(f\"  - For relative comparison: Use '_cv_percent' column\")\n",
    "    print(f\"  - For survey planning: Use combined survey location CSV files\")\n",
    "    \n",
    "    print(f\"\\nALL UNITS ARE IN ORIGINAL SCALE (Ω·m) UNLESS OTHERWISE NOTED\")\n",
    "    print(f\"No log-scale conversions needed for practical use!\")\n",
    "    \n",
    "    print(f\"\\nTotal processing time: {time.time() - start_time:.1f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e212f-e03c-433e-bac5-4b36670d0bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
