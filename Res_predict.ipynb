{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee61eec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy import stats\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "class RandomForestWithUncertainty:\n",
    "    \"\"\"\n",
    "    Random Forest regressor that provides uncertainty estimates\n",
    "    for resistivity prediction in hydrologic surveys\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2, \n",
    "                 min_samples_leaf=1, random_state=42):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.random_state = random_state\n",
    "        self.rf = None\n",
    "        self.is_fitted = False\n",
    "        self.feature_scaler = None\n",
    "        self.output_scaler = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit Random Forest model with scalers\n",
    "        \"\"\"\n",
    "        self.rf = RandomForestRegressor(\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=self.max_depth,\n",
    "            min_samples_split=self.min_samples_split,\n",
    "            min_samples_leaf=self.min_samples_leaf,\n",
    "            random_state=self.random_state,\n",
    "            oob_score=True,  # Enable out-of-bag scoring\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        self.rf.fit(X, y)\n",
    "        self.is_fitted = True\n",
    "        \n",
    "        # Store training data for nearest neighbor uncertainty\n",
    "        self.X_train = X.copy()\n",
    "        self.y_train = y.copy()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_with_uncertainty(self, X, method='ensemble_variance', \n",
    "                                confidence_level=0.95):\n",
    "        \"\"\"\n",
    "        Predict resistivity with uncertainty estimates\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : array, shape (n_samples,)\n",
    "            Mean resistivity predictions\n",
    "        uncertainties : array, shape (n_samples,)\n",
    "            Uncertainty estimates (standard deviation)\n",
    "        prediction_intervals : array, shape (n_samples, 2)\n",
    "            Lower and upper bounds of prediction intervals\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.is_fitted:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        \n",
    "        if method == 'ensemble_variance':\n",
    "            return self._ensemble_variance_uncertainty(X, confidence_level)\n",
    "        elif method == 'nearest_neighbor':\n",
    "            return self._nearest_neighbor_uncertainty(X, confidence_level)\n",
    "        elif method == 'combined':\n",
    "            return self._combined_uncertainty(X, confidence_level)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown uncertainty method: {method}\")\n",
    "    \n",
    "    def _ensemble_variance_uncertainty(self, X, confidence_level):\n",
    "        \"\"\"\n",
    "        Calculate uncertainty from ensemble variance\n",
    "        \"\"\"\n",
    "        # Get predictions from all trees\n",
    "        tree_predictions = np.array([\n",
    "            tree.predict(X) for tree in self.rf.estimators_\n",
    "        ])\n",
    "        \n",
    "        # Calculate statistics\n",
    "        predictions = np.mean(tree_predictions, axis=0)\n",
    "        uncertainties = np.std(tree_predictions, axis=0)\n",
    "        \n",
    "        # Calculate prediction intervals\n",
    "        z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "        margin = z_score * uncertainties\n",
    "        \n",
    "        prediction_intervals = np.column_stack([\n",
    "            predictions - margin,\n",
    "            predictions + margin\n",
    "        ])\n",
    "        \n",
    "        return predictions, uncertainties, prediction_intervals\n",
    "    \n",
    "    def _nearest_neighbor_uncertainty(self, X, confidence_level, k=10):\n",
    "        \"\"\"\n",
    "        Calculate uncertainty based on local neighborhood variance\n",
    "        \"\"\"\n",
    "        # Fit nearest neighbors on training data\n",
    "        nn = NearestNeighbors(n_neighbors=min(k, len(self.X_train)))\n",
    "        nn.fit(self.X_train)\n",
    "        \n",
    "        # Find nearest neighbors for each prediction point\n",
    "        distances, indices = nn.kneighbors(X)\n",
    "        \n",
    "        # Get base predictions\n",
    "        predictions = self.rf.predict(X)\n",
    "        uncertainties = np.zeros(len(X))\n",
    "        \n",
    "        # Calculate local uncertainty for each point\n",
    "        for i in range(len(X)):\n",
    "            # Get predictions for nearest neighbors\n",
    "            neighbor_indices = indices[i]\n",
    "            if hasattr(self.y_train, 'iloc'):\n",
    "                neighbor_targets = self.y_train.iloc[neighbor_indices]\n",
    "            else:\n",
    "                neighbor_targets = self.y_train[neighbor_indices]\n",
    "            \n",
    "            # Use variance of neighbors + distance penalty as uncertainty estimate\n",
    "            local_variance = np.var(neighbor_targets)\n",
    "            distance_penalty = np.mean(distances[i]) * 0.1  # Scale distance penalty\n",
    "            uncertainties[i] = np.sqrt(local_variance + distance_penalty)\n",
    "        \n",
    "        # Calculate prediction intervals\n",
    "        z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "        margin = z_score * uncertainties\n",
    "        \n",
    "        prediction_intervals = np.column_stack([\n",
    "            predictions - margin,\n",
    "            predictions + margin\n",
    "        ])\n",
    "        \n",
    "        return predictions, uncertainties, prediction_intervals\n",
    "    \n",
    "    def _combined_uncertainty(self, X, confidence_level):\n",
    "        \"\"\"\n",
    "        Combine ensemble variance and nearest neighbor uncertainty\n",
    "        \"\"\"\n",
    "        # Get both uncertainty estimates\n",
    "        pred1, unc1, _ = self._ensemble_variance_uncertainty(X, confidence_level)\n",
    "        pred2, unc2, _ = self._nearest_neighbor_uncertainty(X, confidence_level)\n",
    "        \n",
    "        # Use ensemble prediction (more stable)\n",
    "        predictions = pred1\n",
    "        \n",
    "        # Combine uncertainties (weighted average)\n",
    "        uncertainties = np.sqrt(0.7 * unc1**2 + 0.3 * unc2**2)\n",
    "        \n",
    "        # Calculate prediction intervals\n",
    "        z_score = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "        margin = z_score * uncertainties\n",
    "        \n",
    "        prediction_intervals = np.column_stack([\n",
    "            predictions - margin,\n",
    "            predictions + margin\n",
    "        ])\n",
    "        \n",
    "        return predictions, uncertainties, prediction_intervals\n",
    "    \n",
    "    def identify_high_uncertainty_locations(self, X, uncertainty_threshold_percentile=80):\n",
    "        \"\"\"\n",
    "        Identify locations with high uncertainty for targeted sampling\n",
    "        \"\"\"\n",
    "        predictions, uncertainties, _ = self.predict_with_uncertainty(X, method='combined')\n",
    "        \n",
    "        # Define high uncertainty threshold\n",
    "        threshold = np.percentile(uncertainties, uncertainty_threshold_percentile)\n",
    "        high_uncertainty_mask = uncertainties >= threshold\n",
    "        \n",
    "        # Rank locations by uncertainty\n",
    "        uncertainty_ranking = np.argsort(uncertainties)[::-1]  # Highest first\n",
    "        \n",
    "        return high_uncertainty_mask, uncertainty_ranking, uncertainties\n",
    "\n",
    "# Load the terrain attributes CSV file\n",
    "FILE_PATH = \"terrain_attributes_clean.csv\"\n",
    "print(f\"Loading terrain data from {FILE_PATH}...\")\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# Explore the dataset\n",
    "print(\"\\nDataset Info:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "\n",
    "# Identify depth layers (columns starting with 'layer_')\n",
    "depth_layers = [col for col in df.columns if col.startswith('layer_')]\n",
    "print(f\"\\nFound {len(depth_layers)} depth layers: {depth_layers}\")\n",
    "\n",
    "# Feature columns (terrain attributes)\n",
    "feature_columns = ['elevation', 'slope', 'aspect', 'plan_curvature']\n",
    "print(f\"\\nFeature columns: {feature_columns}\")\n",
    "\n",
    "# Function to safely apply log10 transformation to resistivity data\n",
    "def safe_log10_transform(data):\n",
    "    \"\"\"Safely apply log10 transformation to data, handling zeros and negative values.\"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    if isinstance(data, pd.Series):\n",
    "        data_copy = data.copy()\n",
    "    else:\n",
    "        data_copy = np.copy(data)\n",
    "    \n",
    "    # Check for non-positive values\n",
    "    min_val = np.nanmin(data_copy)\n",
    "    offset = 0\n",
    "    \n",
    "    # If we have zero or negative values, add an offset\n",
    "    if min_val <= 0:\n",
    "        offset = abs(min_val) + 1e-10  # Add a small epsilon\n",
    "        data_copy = data_copy + offset\n",
    "        print(f\"Added offset of {offset} before log transform (min value was {min_val})\")\n",
    "    \n",
    "    # Apply log10 transform and handle infinities in one step\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        log_data = np.log10(data_copy)\n",
    "        # Replace infinities with NaN\n",
    "        if isinstance(log_data, np.ndarray):\n",
    "            log_data[np.isinf(log_data)] = np.nan\n",
    "        elif isinstance(log_data, pd.Series):\n",
    "            log_data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    return log_data, offset\n",
    "\n",
    "# Function to inverse the log10 transformation\n",
    "def inverse_log10_transform(log_data, offset):\n",
    "    \"\"\"Inverse the log10 transformation, removing any offset added.\"\"\"\n",
    "    # Handle infinities\n",
    "    if isinstance(log_data, np.ndarray):\n",
    "        clean_log_data = log_data.copy()\n",
    "        clean_log_data[np.isinf(clean_log_data)] = np.nan\n",
    "    elif isinstance(log_data, pd.Series):\n",
    "        clean_log_data = log_data.replace([np.inf, -np.inf], np.nan)\n",
    "    else:\n",
    "        clean_log_data = log_data\n",
    "        \n",
    "    # Apply inverse transform\n",
    "    return 10**clean_log_data - offset\n",
    "\n",
    "# Function to visualize feature importance for a layer\n",
    "def plot_feature_importance(model, feature_columns, layer_name):\n",
    "    \"\"\"Create a feature importance plot for the trained model.\"\"\"\n",
    "    # Get feature importances\n",
    "    importances = model.rf.feature_importances_  # Updated for uncertainty model\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    \n",
    "    # Create a figure for the feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f'Feature Importances for {layer_name}')\n",
    "    plt.bar(range(len(feature_columns)), importances[indices], align='center')\n",
    "    plt.xticks(range(len(feature_columns)), [feature_columns[i] for i in indices], rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f\"{layer_name}_feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved feature importance plot to {layer_name}_feature_importance.png\")\n",
    "\n",
    "# Function to visualize spatial distribution of resistivity with uncertainty\n",
    "def plot_spatial_distribution_with_uncertainty(df, layer_name):\n",
    "    \"\"\"Create spatial distribution plots for predictions and uncertainty.\"\"\"\n",
    "    uncertainty_col = f\"{layer_name}_uncertainty\"\n",
    "    \n",
    "    # Skip if the layer has too many NaN values\n",
    "    if df[layer_name].isna().sum() > 0.9 * len(df):\n",
    "        print(f\"Too many NaN values to create spatial distribution plot for {layer_name}\")\n",
    "        return\n",
    "    \n",
    "    # Make a copy to avoid modifying original data\n",
    "    plot_df = df.copy()\n",
    "    \n",
    "    # Replace any inf values with NaN\n",
    "    for col in [layer_name, uncertainty_col]:\n",
    "        if col in plot_df.columns:\n",
    "            if isinstance(plot_df[col], pd.Series):\n",
    "                plot_df[col] = plot_df[col].replace([np.inf, -np.inf], np.nan)\n",
    "            else:\n",
    "                plot_df.loc[np.isinf(plot_df[col]), col] = np.nan\n",
    "    \n",
    "    # Create subplot for predictions and uncertainty\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\n",
    "    \n",
    "    # Plot 1: Resistivity predictions\n",
    "    valid_mask1 = ~plot_df[layer_name].isna()\n",
    "    if valid_mask1.sum() > 0:\n",
    "        sc1 = ax1.scatter(plot_df.loc[valid_mask1, 'x'], \n",
    "                         plot_df.loc[valid_mask1, 'y'], \n",
    "                         c=plot_df.loc[valid_mask1, layer_name], \n",
    "                         cmap='viridis', alpha=0.7, s=20)\n",
    "        plt.colorbar(sc1, ax=ax1, label=f'Resistivity at {layer_name}')\n",
    "        ax1.set_xlabel('X Coordinate')\n",
    "        ax1.set_ylabel('Y Coordinate')\n",
    "        ax1.set_title(f'Resistivity Predictions - {layer_name}')\n",
    "    \n",
    "    # Plot 2: Uncertainty\n",
    "    if uncertainty_col in plot_df.columns:\n",
    "        valid_mask2 = ~plot_df[uncertainty_col].isna()\n",
    "        if valid_mask2.sum() > 0:\n",
    "            sc2 = ax2.scatter(plot_df.loc[valid_mask2, 'x'], \n",
    "                             plot_df.loc[valid_mask2, 'y'], \n",
    "                             c=plot_df.loc[valid_mask2, uncertainty_col], \n",
    "                             cmap='Reds', alpha=0.7, s=20)\n",
    "            plt.colorbar(sc2, ax=ax2, label=f'Prediction Uncertainty')\n",
    "            ax2.set_xlabel('X Coordinate')\n",
    "            ax2.set_ylabel('Y Coordinate')\n",
    "            ax2.set_title(f'Prediction Uncertainty - {layer_name}')\n",
    "            \n",
    "            # Identify high uncertainty areas\n",
    "            threshold = plot_df[uncertainty_col].quantile(0.8)\n",
    "            high_unc_mask = plot_df[uncertainty_col] >= threshold\n",
    "            if high_unc_mask.sum() > 0:\n",
    "                ax2.scatter(plot_df.loc[high_unc_mask, 'x'], \n",
    "                           plot_df.loc[high_unc_mask, 'y'], \n",
    "                           c='red', marker='x', s=100, alpha=0.8,\n",
    "                           label=f'Top 20% Uncertainty (n={high_unc_mask.sum()})')\n",
    "                ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f\"{layer_name}_spatial_distribution_with_uncertainty.png\", \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved spatial distribution plot to {layer_name}_spatial_distribution_with_uncertainty.png\")\n",
    "\n",
    "# Function to save optimal survey locations\n",
    "def save_optimal_survey_locations(df, layer_name, n_locations=10):\n",
    "    \"\"\"Identify and save optimal locations for future surveys based on uncertainty.\"\"\"\n",
    "    uncertainty_col = f\"{layer_name}_uncertainty\"\n",
    "    \n",
    "    if uncertainty_col not in df.columns:\n",
    "        print(f\"No uncertainty data available for {layer_name}\")\n",
    "        return\n",
    "    \n",
    "    # Get valid uncertainty data\n",
    "    valid_data = df.dropna(subset=[uncertainty_col, 'x', 'y'])\n",
    "    \n",
    "    if len(valid_data) < n_locations:\n",
    "        print(f\"Not enough valid uncertainty data for {layer_name}\")\n",
    "        return\n",
    "    \n",
    "    # Sort by uncertainty (highest first)\n",
    "    optimal_locations = valid_data.nlargest(n_locations, uncertainty_col)\n",
    "    \n",
    "    # Save to CSV\n",
    "    optimal_locations[['x', 'y', uncertainty_col, layer_name]].to_csv(\n",
    "        f\"{layer_name}_optimal_survey_locations.csv\", index=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Saved top {len(optimal_locations)} optimal survey locations for {layer_name}\")\n",
    "    print(f\"Mean uncertainty at optimal locations: {optimal_locations[uncertainty_col].mean():.4f}\")\n",
    "    print(f\"Mean uncertainty overall: {df[uncertainty_col].mean():.4f}\")\n",
    "\n",
    "# Function to train model and predict for a specific depth layer with uncertainty\n",
    "def train_and_predict_for_layer_with_uncertainty(df, layer_name, feature_columns):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\n--- Processing {layer_name} with Uncertainty Estimation ---\")\n",
    "    \n",
    "    # Split data into known and unknown sets\n",
    "    known_data = df.dropna(subset=[layer_name]).copy()\n",
    "    unknown_data = df[df[layer_name].isna()].copy()\n",
    "    \n",
    "    print(f\"Training data size: {known_data.shape[0]} rows\")\n",
    "    print(f\"Prediction data size: {unknown_data.shape[0]} rows\")\n",
    "    \n",
    "    if known_data.shape[0] < 10:\n",
    "        print(f\"WARNING: Not enough training data for {layer_name}. Skipping.\")\n",
    "        return df\n",
    "    \n",
    "    # Further filter known data to only include rows where all features are non-NaN\n",
    "    known_data = known_data.dropna(subset=feature_columns)\n",
    "    print(f\"Training data size after removing NaN features: {known_data.shape[0]} rows\")\n",
    "    \n",
    "    if known_data.shape[0] < 10:\n",
    "        print(f\"WARNING: Not enough non-NaN training data for {layer_name}. Skipping.\")\n",
    "        return df\n",
    "    \n",
    "    # Extract features and target\n",
    "    X = known_data[feature_columns]\n",
    "    y = known_data[layer_name]\n",
    "    \n",
    "    # First apply log10 transformation to the target variable\n",
    "    print(f\"Applying log10 transformation to {layer_name}\")\n",
    "    y_log, log_offset = safe_log10_transform(y)\n",
    "    \n",
    "    # Remove NaN values before scaling\n",
    "    valid_mask = ~np.isnan(y_log)\n",
    "    if not np.any(valid_mask):\n",
    "        print(f\"WARNING: All values became NaN after log transform for {layer_name}. Skipping.\")\n",
    "        return df\n",
    "    \n",
    "    # Extract valid data points\n",
    "    X_valid = X[valid_mask]\n",
    "    y_log_valid = y_log[valid_mask]\n",
    "    \n",
    "    # EXPLICITLY normalize input features to 0-1 range\n",
    "    feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    X_normalized = feature_scaler.fit_transform(X_valid)\n",
    "    \n",
    "    # Print before and after normalization stats\n",
    "    print(\"\\nInput features before normalization:\")\n",
    "    for i, col in enumerate(feature_columns):\n",
    "        col_values = X_valid[col].values\n",
    "        print(f\"  {col}: min={np.min(col_values):.4f}, max={np.max(col_values):.4f}, mean={np.mean(col_values):.4f}\")\n",
    "    \n",
    "    print(\"\\nInput features after normalization:\")\n",
    "    for i, col in enumerate(feature_columns):\n",
    "        col_values = X_normalized[:, i]\n",
    "        print(f\"  {col}: min={np.min(col_values):.4f}, max={np.max(col_values):.4f}, mean={np.mean(col_values):.4f}\")\n",
    "    \n",
    "    # Create output scaler for the log-transformed target variable (0-1 range)\n",
    "    output_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    \n",
    "    # Properly reshape to 2D array for scaling\n",
    "    y_log_valid_2d = y_log_valid.values.reshape(-1, 1)\n",
    "    y_normalized_valid = output_scaler.fit_transform(y_log_valid_2d).flatten()\n",
    "    \n",
    "    # Split the data with valid values only\n",
    "    X_train, X_test, y_train_norm, y_test_norm = train_test_split(\n",
    "        X_normalized, y_normalized_valid, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Also keep track of original values for evaluation\n",
    "    _, _, y_train_orig, y_test_orig = train_test_split(\n",
    "        X_valid, y[valid_mask], test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the RandomForest model with uncertainty estimation\n",
    "    print(\"\\nTraining model with uncertainty estimation...\")\n",
    "    model = RandomForestWithUncertainty(\n",
    "        n_estimators=100, \n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train_norm)\n",
    "    \n",
    "    # Store scalers in model for later use\n",
    "    model.feature_scaler = feature_scaler\n",
    "    model.output_scaler = output_scaler\n",
    "    model.log_offset = log_offset\n",
    "    \n",
    "    # Generate feature importance plot\n",
    "    plot_feature_importance(model, feature_columns, layer_name)\n",
    "    \n",
    "    # Evaluate on test set with uncertainty\n",
    "    y_pred_norm, y_uncertainty_norm, y_intervals_norm = model.predict_with_uncertainty(\n",
    "        X_test, method='combined'\n",
    "    )\n",
    "    \n",
    "    # Convert normalized predictions back to original scale\n",
    "    # First to log scale\n",
    "    y_pred_log = output_scaler.inverse_transform(y_pred_norm.reshape(-1, 1)).flatten()\n",
    "    y_uncertainty_log = y_uncertainty_norm * (output_scaler.data_max_ - output_scaler.data_min_)\n",
    "    \n",
    "    # Then to original scale\n",
    "    y_pred_orig = inverse_log10_transform(y_pred_log, log_offset)\n",
    "    \n",
    "    # Calculate metrics using original scale\n",
    "    mse = mean_squared_error(y_test_orig, y_pred_orig)\n",
    "    r2 = r2_score(y_test_orig, y_pred_orig)\n",
    "    mean_uncertainty = np.mean(y_uncertainty_norm)\n",
    "\n",
    "    \n",
    "    print(f\"Model evaluation (original scale):\")\n",
    "    print(f\"Mean Squared Error: {np.sqrt(mse):.4f}\")\n",
    "    print(f\"RÂ² Score: {r2:.4f}\")\n",
    "    print(f\"Mean Prediction Uncertainty (normalized): {mean_uncertainty:.4f}\")\n",
    "    \n",
    "    # Calculate uncertainty calibration (what % of true values fall within prediction intervals)\n",
    "    y_intervals_log = output_scaler.inverse_transform(y_intervals_norm.reshape(-1, 2))\n",
    "    y_intervals_orig = np.column_stack([\n",
    "        inverse_log10_transform(y_intervals_log[:, 0], log_offset),\n",
    "        inverse_log10_transform(y_intervals_log[:, 1], log_offset)\n",
    "    ])\n",
    "    \n",
    "    coverage = np.mean((y_test_orig >= y_intervals_orig[:, 0]) & \n",
    "                      (y_test_orig <= y_intervals_orig[:, 1]))\n",
    "    print(f\"Prediction Interval Coverage: {coverage:.2%} (target: 95%)\")\n",
    "    \n",
    "    # Predict unknown values with uncertainty\n",
    "    if unknown_data.shape[0] > 0:\n",
    "        # Create a mask for rows with all non-NaN features\n",
    "        valid_features_mask = unknown_data[feature_columns].notna().all(axis=1)\n",
    "        print(f\"Found {valid_features_mask.sum()} out of {unknown_data.shape[0]} unknown rows with all features non-NaN\")\n",
    "        \n",
    "        # Only make predictions for rows with all non-NaN features\n",
    "        if valid_features_mask.sum() > 0:\n",
    "            # Extract data for prediction\n",
    "            valid_unknown_data = unknown_data[valid_features_mask]\n",
    "            X_unknown = valid_unknown_data[feature_columns]\n",
    "            \n",
    "            # Normalize the unknown features using the same scaler\n",
    "            X_unknown_normalized = feature_scaler.transform(X_unknown)\n",
    "            \n",
    "            # Make predictions with uncertainty (in normalized space)\n",
    "            unknown_pred_norm, unknown_uncertainty_norm, unknown_intervals_norm = model.predict_with_uncertainty(\n",
    "                X_unknown_normalized, method='combined'\n",
    "            )\n",
    "            \n",
    "            # Convert predictions back to original scale\n",
    "            # First to log scale\n",
    "            unknown_pred_log = output_scaler.inverse_transform(\n",
    "                unknown_pred_norm.reshape(-1, 1)\n",
    "            ).flatten()\n",
    "            \n",
    "            # Then to original scale\n",
    "            unknown_predictions = inverse_log10_transform(unknown_pred_log, log_offset)\n",
    "            \n",
    "            # Scale uncertainty back to log scale\n",
    "            unknown_uncertainty_log = unknown_uncertainty_norm * (output_scaler.data_max_ - output_scaler.data_min_)\n",
    "            \n",
    "            # Create a copy of the original dataframe\n",
    "            updated_df = df.copy()\n",
    "            \n",
    "            # Update only the NaN values with predictions for rows where all features are non-NaN\n",
    "            mask = updated_df[layer_name].isna() & updated_df[feature_columns].notna().all(axis=1)\n",
    "            updated_df.loc[mask, layer_name] = unknown_predictions\n",
    "            updated_df.loc[mask, f\"{layer_name}_uncertainty\"] = unknown_uncertainty_log\n",
    "            \n",
    "            print(f\"Filled {sum(mask)} missing values for {layer_name}\")\n",
    "            print(f\"Left {sum(updated_df[layer_name].isna())} values as NaN (either target or features had NaN)\")\n",
    "            \n",
    "            # Generate the spatial distribution plot with uncertainty\n",
    "            plot_spatial_distribution_with_uncertainty(updated_df, layer_name)\n",
    "            \n",
    "            # Save optimal survey locations\n",
    "            save_optimal_survey_locations(updated_df, layer_name, n_locations=10)\n",
    "            \n",
    "            print(f\"Processing time: {time.time() - start_time:.2f} seconds\")\n",
    "            return updated_df\n",
    "        else:\n",
    "            print(f\"No valid rows with all non-NaN features to predict for {layer_name}\")\n",
    "            return df\n",
    "    else:\n",
    "        print(f\"No missing values to predict for {layer_name}\")\n",
    "        return df\n",
    "\n",
    "# Process each depth layer with uncertainty estimation\n",
    "updated_df = df.copy()\n",
    "\n",
    "for layer in depth_layers:\n",
    "    updated_df = train_and_predict_for_layer_with_uncertainty(updated_df, layer, feature_columns)\n",
    "\n",
    "# Save the updated dataset with predictions and uncertainties\n",
    "output_file = \"terrain_with_predicted_resistivity_and_uncertainty.csv\"\n",
    "updated_df.to_csv(output_file, index=False)\n",
    "print(f\"\\nSaved complete dataset with predictions and uncertainties to {output_file}\")\n",
    "\n",
    "# Print summary of filled values and uncertainty statistics\n",
    "print(\"\\nSummary of filled values and uncertainty:\")\n",
    "for layer in depth_layers:\n",
    "    original_nan_count = df[layer].isna().sum()\n",
    "    remaining_nan_count = updated_df[layer].isna().sum()\n",
    "    filled_count = original_nan_count - remaining_nan_count\n",
    "    total_count = len(df)\n",
    "    \n",
    "    uncertainty_col = f\"{layer}_uncertainty\"\n",
    "    if uncertainty_col in updated_df.columns:\n",
    "        mean_uncertainty = updated_df[uncertainty_col].mean()\n",
    "        max_uncertainty = updated_df[uncertainty_col].max()\n",
    "        print(f\"{layer}: Filled {filled_count}/{original_nan_count} missing values ({filled_count/total_count*100:.1f}% of total)\")\n",
    "        print(f\"  Mean uncertainty: {mean_uncertainty:.4f}, Max uncertainty: {max_uncertainty:.4f}\")\n",
    "    else:\n",
    "        print(f\"{layer}: Filled {filled_count}/{original_nan_count} missing values ({filled_count/total_count*100:.1f}% of total)\")\n",
    "\n",
    "# Create summary report for ModEx framework\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEX FRAMEWORK SUMMARY REPORT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataset Overview:\")\n",
    "print(f\"Total locations: {len(updated_df)}\")\n",
    "print(f\"Depth layers processed: {len(depth_layers)}\")\n",
    "print(f\"Features used: {feature_columns}\")\n",
    "\n",
    "for layer in depth_layers:\n",
    "    uncertainty_col = f\"{layer}_uncertainty\"\n",
    "    if uncertainty_col in updated_df.columns:\n",
    "        print(f\"\\n{layer.upper()} - ModEx Recommendations:\")\n",
    "        \n",
    "        # High uncertainty locations for targeted surveys\n",
    "        high_unc_threshold = updated_df[uncertainty_col].quantile(0.8)\n",
    "        high_unc_count = (updated_df[uncertainty_col] >= high_unc_threshold).sum()\n",
    "        \n",
    "        print(f\"  High uncertainty locations (top 20%): {high_unc_count}\")\n",
    "        print(f\"  Recommended for Phase 5 field campaigns\")\n",
    "        print(f\"  Priority: {'HIGH' if high_unc_count > 50 else 'MEDIUM' if high_unc_count > 20 else 'LOW'}\")\n",
    "        \n",
    "        # Coverage assessment\n",
    "        predicted_count = updated_df[layer].notna().sum()\n",
    "        coverage_pct = predicted_count / len(updated_df) * 100\n",
    "        print(f\"  Spatial coverage: {coverage_pct:.1f}%\")\n",
    "        print(f\"  Model confidence: {'HIGH' if coverage_pct > 80 else 'MEDIUM' if coverage_pct > 60 else 'LOW'}\")\n",
    "\n",
    "print(f\"\\nNext Steps for ModEx Cycle:\")\n",
    "print(f\"1. Review optimal survey locations CSV files\")\n",
    "print(f\"2. Plan Phase 5 field campaigns for high uncertainty areas\") \n",
    "print(f\"3. Collect new resistivity data at recommended locations\")\n",
    "print(f\"4. Retrain models (Phase 6) with new data to reduce uncertainty\")\n",
    "print(f\"5. Iterate until acceptable uncertainty levels achieved\")\n",
    "\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
